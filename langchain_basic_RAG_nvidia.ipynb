{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5fb0b9e-f9cd-404f-bd8d-0273e94ac1fe",
   "metadata": {
    "id": "c5fb0b9e-f9cd-404f-bd8d-0273e94ac1fe"
   },
   "source": [
    "# Part 1 pre-acceptance workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2969cdab-82fc-4ce5-bde1-b4f629691f27",
   "metadata": {
    "id": "2969cdab-82fc-4ce5-bde1-b4f629691f27"
   },
   "source": [
    "This notebook is meant to be a prototype for the NVIDIA NIM hackathon project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca300278-5ff4-47c4-ab70-c6584ef73c9f",
   "metadata": {
    "id": "ca300278-5ff4-47c4-ab70-c6584ef73c9f"
   },
   "source": [
    "## Installation and Requirements\n",
    "\n",
    "Shits fucked cunt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de8e53a-7940-4e72-87f3-e1c014128806",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "5de8e53a-7940-4e72-87f3-e1c014128806",
    "outputId": "2f26a4cb-ef97-4590-c9c8-c1118e1f3b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.2.5 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (0.2.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain==0.2.5) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain==0.2.5) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain==0.2.5) (3.10.9)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.7 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain==0.2.5) (0.2.43)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain==0.2.5) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain==0.2.5) (0.1.131)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain==0.2.5) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain==0.2.5) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain==0.2.5) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain==0.2.5) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (1.13.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.7->langchain==0.2.5) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.7->langchain==0.2.5) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.7->langchain==0.2.5) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.5) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.5) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.5) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain==0.2.5) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain==0.2.5) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from requests<3,>=2->langchain==0.2.5) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from requests<3,>=2->langchain==0.2.5) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from requests<3,>=2->langchain==0.2.5) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from requests<3,>=2->langchain==0.2.5) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.5) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.5) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.5) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.5) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.5) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain==0.2.5) (3.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community==0.2.5 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (0.2.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain_community==0.2.5) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain_community==0.2.5) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain_community==0.2.5) (3.10.9)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain_community==0.2.5) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.5 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain_community==0.2.5) (0.2.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.7 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain_community==0.2.5) (0.2.43)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain_community==0.2.5) (0.1.131)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain_community==0.2.5) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain_community==0.2.5) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain_community==0.2.5) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.2.5) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.2.5) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.2.5) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.2.5) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.2.5) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.2.5) (1.13.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.2.5) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.2.5) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain<0.3.0,>=0.2.5->langchain_community==0.2.5) (0.2.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain<0.3.0,>=0.2.5->langchain_community==0.2.5) (2.9.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.7->langchain_community==0.2.5) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.7->langchain_community==0.2.5) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.7->langchain_community==0.2.5) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community==0.2.5) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community==0.2.5) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community==0.2.5) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community==0.2.5) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community==0.2.5) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community==0.2.5) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community==0.2.5) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community==0.2.5) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community==0.2.5) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community==0.2.5) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community==0.2.5) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community==0.2.5) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain_community==0.2.5) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain_community==0.2.5) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain_community==0.2.5) (2.23.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community==0.2.5) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '#'\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from faiss-cpu) (24.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-nvidia-ai-endpoints==0.1.2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain-nvidia-ai-endpoints==0.1.2) (3.10.9)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.27 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain-nvidia-ai-endpoints==0.1.2) (0.2.43)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.0.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain-nvidia-ai-endpoints==0.1.2) (10.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints==0.1.2) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints==0.1.2) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints==0.1.2) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints==0.1.2) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints==0.1.2) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints==0.1.2) (1.13.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (0.1.131)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (3.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (2.23.4)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints==0.1.2) (3.10)\n",
      "Requirement already satisfied: anyio in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (4.6.0)\n",
      "Requirement already satisfied: certifi in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.27->langchain-nvidia-ai-endpoints==0.1.2) (2.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: pdfplumber in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (0.11.4)\n",
      "Requirement already satisfied: spacy in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: camelot-py in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from pdfplumber) (10.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from spacy) (0.13.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: chardet>=3.0.4 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from camelot-py) (5.2.0)\n",
      "Requirement already satisfied: click>=6.7 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from camelot-py) (8.1.7)\n",
      "Requirement already satisfied: openpyxl>=2.5.8 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from camelot-py) (3.1.5)\n",
      "Requirement already satisfied: pandas>=0.23.4 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from camelot-py) (2.2.3)\n",
      "Requirement already satisfied: pypdf>=3.0.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from camelot-py) (5.0.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from camelot-py) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from click>=6.7->camelot-py) (0.4.6)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: et-xmlfile in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from openpyxl>=2.5.8->camelot-py) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from pandas>=0.23.4->camelot-py) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from pandas>=0.23.4->camelot-py) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from pandas>=0.23.4->camelot-py) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Obtaining dependency information for numpy>=1.19.0 from https://files.pythonhosted.org/packages/b2/b5/4ac39baebf1fdb2e72585c8352c56d063b6126be9fc95bd2bb5ef5770c20/numpy-2.0.2-cp312-cp312-win_amd64.whl.metadata\n",
      "  Using cached numpy-2.0.2-cp312-cp312-win_amd64.whl.metadata (59 kB)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.23.4->camelot-py) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Using cached numpy-2.0.2-cp312-cp312-win_amd64.whl (15.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\GERARD\\\\Coding\\\\RAG\\\\project\\\\venv\\\\Lib\\\\site-packages\\\\~-mpy.libs\\\\libopenblas64__v0.3.23-293-gc2f4bdbb-gcc_10_3_0-2bde3a66a51006b2b53eb373ff767a3f.dll'\n",
      "Check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from beautifulsoup4) (2.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (1.24.11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (5.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unstructured in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (0.15.13)\n",
      "Requirement already satisfied: chardet in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (5.2.0)\n",
      "Requirement already satisfied: filetype in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: python-magic in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: lxml in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (5.3.0)\n",
      "Requirement already satisfied: nltk in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (3.9.1)\n",
      "Requirement already satisfied: tabulate in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (0.9.0)\n",
      "Requirement already satisfied: requests in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (4.12.3)\n",
      "Requirement already satisfied: emoji in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (2.14.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (2024.4.27)\n",
      "Requirement already satisfied: langdetect in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (1.0.9)\n",
      "Collecting numpy<2 (from unstructured)\n",
      "  Obtaining dependency information for numpy<2 from https://files.pythonhosted.org/packages/16/2e/86f24451c2d530c88daf997cb8d6ac622c1d40d19f5a031ed68a4b73a374/numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: rapidfuzz in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (3.10.0)\n",
      "Requirement already satisfied: backoff in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (4.12.2)\n",
      "Requirement already satisfied: unstructured-client in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (0.25.9)\n",
      "Requirement already satisfied: wrapt in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (1.16.0)\n",
      "Requirement already satisfied: tqdm in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (4.66.5)\n",
      "Requirement already satisfied: psutil in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (6.0.0)\n",
      "Requirement already satisfied: python-oxmsg in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured) (0.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from beautifulsoup4->unstructured) (2.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from dataclasses-json->unstructured) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from langdetect->unstructured) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from nltk->unstructured) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from nltk->unstructured) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from nltk->unstructured) (2024.9.11)\n",
      "Requirement already satisfied: olefile in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from python-oxmsg->unstructured) (0.47)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from requests->unstructured) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from requests->unstructured) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from requests->unstructured) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from requests->unstructured) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from tqdm->unstructured) (0.4.6)\n",
      "Requirement already satisfied: cryptography>=3.1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured-client->unstructured) (43.0.1)\n",
      "Requirement already satisfied: deepdiff>=6.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured-client->unstructured) (8.0.1)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured-client->unstructured) (0.27.0)\n",
      "Requirement already satisfied: jsonpath-python>=1.0.6 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured-client->unstructured) (1.0.6)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Requirement already satisfied: packaging>=23.1 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured-client->unstructured) (24.1)\n",
      "Requirement already satisfied: pypdf>=4.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured-client->unstructured) (5.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured-client->unstructured) (2.9.0.post0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
      "Requirement already satisfied: orderly-set==5.2.2 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from deepdiff>=6.0->unstructured-client->unstructured) (5.2.2)\n",
      "Requirement already satisfied: anyio in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
      "Requirement already satisfied: pycparser in c:\\gerard\\coding\\rag\\project\\venv\\lib\\site-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "blis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Requirements\n",
    "!pip install langchain==0.2.5\n",
    "!pip install langchain_community==0.2.5\n",
    "!pip install faiss-gpu # replace with faiss-gpu if you are using GPU\n",
    "!pip install faiss-cpu\n",
    "!pip install langchain-nvidia-ai-endpoints==0.1.2\n",
    "!pip install requests pdfplumber spacy camelot-py \n",
    "!pip install pandas==2 numpy==1.26.4 \n",
    "!pip install beautifulsoup4 \n",
    "!pip install pymupdf\n",
    "!pip install lxml\n",
    "!pip install unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7a52a0-7e5e-4064-9665-cb947d600f84",
   "metadata": {
    "id": "1b7a52a0-7e5e-4064-9665-cb947d600f84"
   },
   "source": [
    "## Getting Started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04495732-c2db-4c97-91d0-96708814334d",
   "metadata": {
    "id": "04495732-c2db-4c97-91d0-96708814334d"
   },
   "source": [
    "You need an `NVIDIA_API_KEY` to use the NVIDIA API Catalog:\n",
    "\n",
    "1) Create a free account with [NVIDIA](https://build.nvidia.com/explore/discover).\n",
    "2) Click on your model of choice.\n",
    "3) Under Input select the Python tab, and click **Get API Key** and then click **Generate Key**.\n",
    "4) Copy and save the generated key as NVIDIA_API_KEY. From there, you should have access to the endpoints.\n",
    "5) If at any point downstream NVIDIA complains about insufficient credits, tell them to shut the fuck up and make a new gmail. If their stock price is so high, they should share some of those profits with me, an NVIDIA investor (i own 3 shares rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb51115-79f8-48c3-b3ee-d434916945f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbb51115-79f8-48c3-b3ee-d434916945f6",
    "outputId": "e4d2f493-998c-42ab-8d7e-60c7cffcd318"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "assert nvidia_api_key.startswith(\"nvapi-\"), f\"{nvidia_api_key[:5]}... is not a valid key\"\n",
    "os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25656ab5-0046-4e27-be65-b3d3d547b4c6",
   "metadata": {
    "id": "25656ab5-0046-4e27-be65-b3d3d547b4c6"
   },
   "source": [
    "## LLM & Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e86bc0-e9c5-4a2b-be0e-7fca0331e886",
   "metadata": {
    "id": "54e86bc0-e9c5-4a2b-be0e-7fca0331e886"
   },
   "source": [
    "### 1) Initialize the LLM\n",
    "\n",
    "The ChatNVIDIA class is part of LangChain's integration (langchain_nvidia_ai_endpoints) with NVIDIA NIM microservices.\n",
    "It allows access to NVIDIA NIM for chat applications, connecting to hosted or locally-deployed microservices.\n",
    "\n",
    "Here we will use **mixtral-8x7b-instruct-v0.1**\n",
    "\n",
    "Note: You can use any model hosted at the NVIDIA API catalog using 'ChatNVIDIA.get_available_models()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88c2fafe-5ded-4238-82de-f094232bf6fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "88c2fafe-5ded-4238-82de-f094232bf6fb",
    "outputId": "fa6a7c4a-2ddc-4d73-f511-859bce8769f2"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\", max_tokens=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19acd7b4",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- In this notebook, we have used NVIDIA NIM microservices from the NVIDIA API Catalog.\n",
    "- The other APIs, ChatNVIDIA, NVIDIAEmbedding, and NVIDIARerank, also support self-hosted NIM microservices.\n",
    "- Change the `base_url` to your deployed NIM URL.\n",
    "- Example: `llm = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama3-8b-instruct\")`\n",
    "- NIM can be also hosted locally using Docker, following the [NVIDIA NIM for LLMs](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html) documentation. This is only true if you are the son of a rich oil tycoon, and have a few H100s sitting around in your basement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dcf372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# connect to an LLM NIM running at localhost:8000, specifying a specific model\n",
    "llm = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama3-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cc87a6-2f83-4652-95f1-cf349db8bad6",
   "metadata": {
    "id": "35cc87a6-2f83-4652-95f1-cf349db8bad6"
   },
   "source": [
    "### 2) Intialize the embedding\n",
    "NVIDIAEmbeddings is a client to NVIDIA embeddings models that provides access to a NVIDIA NIM for embedding. It can connect to a hosted NIM or a local NIM using a base URL\n",
    "\n",
    "We selected **NV-Embed-QA** as the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d88f7838-b9f9-4fc5-8779-84df6cb26017",
   "metadata": {
    "id": "d88f7838-b9f9-4fc5-8779-84df6cb26017"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "embedder = NVIDIAEmbeddings(model=\"NV-Embed-QA\", truncate=\"END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9862f2e-5055-4fe4-818d-708091243d74",
   "metadata": {
    "id": "b9862f2e-5055-4fe4-818d-708091243d74"
   },
   "source": [
    "### 3) Obtain dataset\n",
    "I love taxes and work! Lets steal information about taxes and work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64106ff",
   "metadata": {
    "id": "b64106ff"
   },
   "source": [
    "#### In the **DataHandler** class defined below, we can;\n",
    "\n",
    "A) Walk through a webpage and find all sub-webpages and scrape the parent and children,\n",
    "\n",
    "B) Extract texts, tables and images from pdfs\n",
    "\n",
    "C) Split documents \n",
    "\n",
    "- Real world documents can be very long, this makes it hard to fit in the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
    "\n",
    "- To handle this we’ll split the Document into chunks for embedding and vector storage. More on text splitting [here](https://python.langchain.com/v0.2/docs/concepts/#text-splitters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c07d429",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "8c07d429",
    "outputId": "86e892d9-a219-4dd6-8a18-71daf5ee84e9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.parse  # To handle URL joining\n",
    "import fitz\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader, DataFrameLoader, CSVLoader, UnstructuredTSVLoader\n",
    "\n",
    "class DataHandler:\n",
    "    \"\"\"\n",
    "    Masterfully handles data scraping, preprocessing, and other data-related functionalities in this notebook.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path=\"./data/csv\", pdf_path=\"./data/pdf\"):\n",
    "        # Paths and storage\n",
    "        self.csv_path = csv_path\n",
    "        self.pdf_path = pdf_path\n",
    "        self.visited_urls = set()\n",
    "\n",
    "        # Data containers\n",
    "        self.raw_data = []\n",
    "        self.textual_data = []\n",
    "        self.textual_metadata = []\n",
    "        self.tabular_data = []\n",
    "        self.tabular_metadata = []\n",
    "        self.visual_data = []\n",
    "        self.visual_metadata = []\n",
    "        \n",
    "        # Final data containers\n",
    "        self.embedded_data = []\n",
    "        self.metadata = []\n",
    "\n",
    "        # Text splitter\n",
    "        self.text_splitter = CharacterTextSplitter(chunk_size=2048, separator=\" \", chunk_overlap=64)\n",
    "\n",
    "        # Ensure directories exist\n",
    "        os.makedirs(self.csv_path, exist_ok=True)\n",
    "        os.makedirs(self.pdf_path, exist_ok=True)\n",
    "        \n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        \"\"\"\n",
    "        Cleans text to reduce storage and improve readability\n",
    "        \n",
    "        Args:\n",
    "            - text (str):   text to clean\n",
    "        \"\"\"\n",
    "        return \" \".join(text.split())\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_text(text, similarity_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Filters through mass amounts of data to see if there is relevance to our use case\n",
    "\n",
    "        Args:\n",
    "            - text (str):   text to check and filter through\n",
    "        \"\"\"\n",
    "        keywords = [ \"law\", \"compliance\", \"regulation\", \"tax\", \"contract\", \"finance\", \"employment\", \"business\", \"corporate\", \"trade\", \"policy\", \"license\", \"permits\", \"registration\", \"audit\", \"liability\", \"shareholders\", \"partnership\", \"incorporation\", \"startup\", \"profit\", \"revenue\", \"penalty\", \"customs\", \"dispute\", \"governance\", \"authority\", \"import\", \"export\", \"data\", \"privacy\", \"director\", \"management\", \"intellectual\", \"property\", \"trademark\", \"compliance\", \"ownership\", \"capital\", \"dividend\", \"funding\", \"duty\", \"penalty\", \"fine\", \"subsidiary\", \"merger\", \"acquisition\", \"bankruptcy\", \"insolvency\"]\n",
    "\n",
    "        def cosine_similarity(vec1, vec2):\n",
    "            \"\"\"\n",
    "            Computes the cosine similarity between two vectors and returns cosine similarity score\n",
    "\n",
    "            Args:\n",
    "                - vec1: First embedding vector\n",
    "                - vec2: Second embedding vector\n",
    "            \"\"\"\n",
    "            return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)) \n",
    "    \n",
    "        words = set(text.lower().split())\n",
    "        matches = set(keywords).intersection(words)\n",
    "        keyword_score = len(matches) / len(words) if len(words) > 0 else 0\n",
    "        \n",
    "        if len(text) > 0:\n",
    "            query_embedding = embedder.embed_query(\"Relevant and important information about corporate laws, compliance requirements, tax obligations, and regulatory guidelines for Small and Medium Enterprises (SMEs) operating in Singapore.\")\n",
    "            text_embedding = embedder.embed_query(text)\n",
    "            similarity_score = cosine_similarity(query_embedding, text_embedding)\n",
    "        else:\n",
    "            similarity_score = 0\n",
    "\n",
    "        return (0.1 * keyword_score) + (0.9 * similarity_score) > 0.5\n",
    "\n",
    "    @staticmethod\n",
    "    def embed_text(text):\n",
    "        \"\"\"\n",
    "        Embeds a given text if necessary\n",
    "\n",
    "        Args:\n",
    "            - text (str):   text to embed\n",
    "        \"\"\"\n",
    "        return embedder.embed_query(text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_csv(csv_path: str) -> CSVLoader:\n",
    "        \"\"\"\n",
    "        Reads a CSV file and returns a LangChain CSVLoader instance.\n",
    "\n",
    "        Args:\n",
    "            - csv_path (str):   path to csv file\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(csv_path)\n",
    "        return CSVLoader(\n",
    "            file_path=csv_path,\n",
    "            csv_args={\"delimiter\": \",\", \"quotechar\": '\"', \"fieldnames\": df.columns.tolist()}\n",
    "        )\n",
    "    \n",
    "    def scrape_csvs(self, directory: str):\n",
    "        \"\"\"\n",
    "        Adds CSV files from a directory to the tabular data loaders.\n",
    "\n",
    "        Args:\n",
    "            - directory (str):  directory of csv files\n",
    "        \"\"\"\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    loader = self.read_csv(file_path)\n",
    "                    for row in loader.load()[1:]:\n",
    "                        if self.filter_text(row.page_content):\n",
    "                            split_data = self.text_splitter.split_text(self.clean_text(row.page_content))\n",
    "\n",
    "                            self.tabular_data.extend(split_data)\n",
    "                            self.tabular_metadata.extend([{\"source\": file, \"date_added\": \"None\"} for _ in split_data])\n",
    "                            print(f\"Current csv: {file}\")\n",
    "\n",
    "\n",
    "    def extract_table_elements(self, url, tables):\n",
    "        \"\"\"\n",
    "        Extracts tabular data from HTML tables and saves them as CSV and loaders\n",
    "\n",
    "        Args:\n",
    "            - url (str):        url the table is from\n",
    "            - tables (list):    list of table elements\n",
    "        \"\"\"\n",
    "        for idx, table in enumerate(tables):\n",
    "            try:\n",
    "                # Clean up table superscripts\n",
    "                for tag in table.find_all(\"sup\"):\n",
    "                    tag.extract()\n",
    "\n",
    "                # Extract the table name\n",
    "                header = table.find_previous([\"h3\", \"h4\"])\n",
    "                table_name = self.clean_text(header.text) if header and header.text else f\"table_{idx}\"\n",
    "                table_name = os.path.basename(url) + f\" {table_name}\"\n",
    "\n",
    "                # Parse table into a DataFrame\n",
    "                df = pd.read_html(StringIO(str(table)), header=0)[0]\n",
    "                df[\"context\"] = table_name\n",
    "                for index, row in df.iterrows():\n",
    "                    row_context = [f\"Row {idx+1} - {col}: {str(row[col])}\" for col in df.columns]\n",
    "                    full_context = \" | \".join(row_context)  # Join all columns with a separator to keep context\n",
    "                    if self.filter_text(full_context):\n",
    "                        split_data = self.text_splitter.split_text(self.clean_text(full_context))\n",
    "\n",
    "                        self.tabular_data.extend(split_data)\n",
    "                        self.tabular_metadata.extend([{\"source\": url, \"date_added\": \"None\"} for _ in split_data])\n",
    "                        print(f\"Current table: {table_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to extract table from {url}: {e}\")\n",
    "\n",
    "    def create_loaders(self):\n",
    "        \"\"\"\n",
    "        Processes raw data to create LangChain loaders\n",
    "\n",
    "        Args:   None\n",
    "        \"\"\"\n",
    "        for url, soup in self.raw_data:\n",
    "            main_content = soup.find(\"main\") \n",
    "            if main_content:\n",
    "                raw_text = main_content.get_text(separator=\" \", strip=True)\n",
    "                if self.filter_text(raw_text):\n",
    "                    split_data = self.text_splitter.split_text(self.clean_text(raw_text))\n",
    "\n",
    "                    self.textual_data.extend(split_data)\n",
    "                    self.textual_metadata.extend([{\"source\": url, \"date_added\": \"None\"} for _ in split_data])\n",
    "\n",
    "    def scrape_websites(self, urls, max_depth, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively scrapes a website for HTML content and tables and then processes the data into loaders\n",
    "\n",
    "        Args:\n",
    "            - urls (list):      list of urls to scrape from\n",
    "            - max_depth (int):  how many sublinks into the main website we wish to mdig through\n",
    "            - depth (int):      current/starting depth of sublinks reached\n",
    "        \"\"\"\n",
    "        for base_url in urls:\n",
    "            def _scrape(url, depth):\n",
    "                if url in self.visited_urls or depth > max_depth:\n",
    "                    return\n",
    "\n",
    "                try:\n",
    "                    response = requests.get(url)\n",
    "                    if response.status_code != 200:\n",
    "                        print(f\"Failed to retrieve {url}\")\n",
    "                        return\n",
    "\n",
    "                    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    self.visited_urls.add(url)\n",
    "                    self.raw_data.append((url, soup))\n",
    "\n",
    "                    # Process tables\n",
    "                    self.extract_table_elements(url, soup.find_all(\"table\"))\n",
    "\n",
    "                    # Recurse through links\n",
    "                    for link in soup.find_all(\"a\", href=True):\n",
    "                        abs_url = urllib.parse.urljoin(url, link[\"href\"])\n",
    "                        if base_url in abs_url:\n",
    "                            _scrape(abs_url, depth + 1)\n",
    "                    print(f\"Current url: {url}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error accessing {url}: {e}\")\n",
    "\n",
    "            _scrape(base_url, depth)\n",
    "            self.create_loaders()\n",
    "\n",
    "    def scrape_pdfs(self, pdf_folder=None):\n",
    "        \"\"\"\n",
    "        Extracts text from PDF files.\n",
    "\n",
    "        Args:\n",
    "            - pdf_folder (str): folder of pdf files\n",
    "        \"\"\"\n",
    "        pdf_folder = pdf_folder or self.pdf_path\n",
    "        for file in os.listdir(pdf_folder):\n",
    "            if file.endswith(\".pdf\"):\n",
    "                pdf_path = os.path.join(pdf_folder, file)\n",
    "                try:\n",
    "                    doc = fitz.open(pdf_path)\n",
    "                    pdf_text = \"\".join(page.get_text(\"text\") for page in doc)\n",
    "                    if self.filter_text(pdf_text):\n",
    "                        split_text = self.text_splitter.split_text(self.clean_text(pdf_text))\n",
    "\n",
    "                        self.textual_data.extend(split_text)\n",
    "                        self.textual_metadata.extend([{\"source\": pdf_path, \"date_added\": \"None\"} for _ in split_text])\n",
    "                        print(f\"Current pdf: {pdf_path}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process {pdf_path}: {e}\")\n",
    "                \n",
    "    def prepare_data_for_insertion(self):\n",
    "        \"\"\"\n",
    "        Prepares data before data can be added into the vector db\n",
    "\n",
    "        Args:   None\n",
    "        \"\"\"\n",
    "        self.embedded_data = [self.embed_text(text) for text in tqdm(self.textual_data, desc=f\"Total pieces of textual data: {len(self.textual_data)}\")] + [self.embed_text(text) for text in tqdm(self.tabular_data, desc=f\"Total pieces of tabular data: {len(self.tabular_data)}\")]\n",
    "        self.metadata = self.textual_metadata + self.tabular_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec1d94c",
   "metadata": {},
   "source": [
    "We define our websites and pdf links below to call the dataloader on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e340c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = [\n",
    "    \"https://www.iras.gov.sg\",\n",
    "    \"https://www.mom.gov.sg\",\n",
    "    \"https://www.acra.gov.sg\",\n",
    "    \"https://singaporelegaladvice.com\",\n",
    "    \"https://www.ipos.gov.sg\",\n",
    "    \"https://www.enterprisesg.gov.sg\",\n",
    "    \"https://www.skillsfuture.gov.sg\",\n",
    "    \"https://www.hsa.gov.sg\",\n",
    "    \"https://www.sfa.gov.sg\"\n",
    "]\n",
    "\n",
    "datahandler = DataHandler()\n",
    "datahandler.scrape_websites(websites, max_depth=5)\n",
    "datahandler.scrape_pdfs('./data/pdfs')\n",
    "datahandler.scrape_csvs('./data/csvs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9a8d2",
   "metadata": {},
   "source": [
    "And then, we embed the textual and tabular data gathered for use in the vector db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b763dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding textual data...: 100%|██████████| 7/7 [00:03<00:00,  1.75it/s]\n",
      "Embedding tabular data...: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "datahandler.prepare_data_for_insertion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b16b3-43ac-4269-9f37-05a33efe24fb",
   "metadata": {
    "id": "851b16b3-43ac-4269-9f37-05a33efe24fb"
   },
   "source": [
    "### 4) Storing the documents\n",
    "\n",
    "To build our foundational knowledge base from our collected data and allow for faster retrieval of vector queries, we need to have some form of search system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687a9681",
   "metadata": {},
   "source": [
    "#### a) Process the documents into vectorstore and save it to disk\n",
    "\n",
    "Vectorstores are good when we wish to store small datasets mainly in memory, thus using less storage.\n",
    "\n",
    "In this case, we use FAISS, which is a high-performace library that is efficient for similarity search and clustering of dense vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c85f6-181b-4291-a685-d6b378015544",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "804c85f6-181b-4291-a685-d6b378015544",
    "outputId": "ec8742d2-d131-4dfc-ab1e-854694194e0e"
   },
   "outputs": [],
   "source": [
    "# Here we create a faiss vector store from the documents and save it to disk.\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# You will only need to do this once, later on we will restore the already saved vectorstore\n",
    "store = FAISS.from_texts(datahandler.textual_data, embedder)\n",
    "VECTOR_STORE = './data/nv_embedding'\n",
    "store.save_local(VECTOR_STORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f867df18-11c8-45ea-b81c-1603459431f9",
   "metadata": {
    "id": "f867df18-11c8-45ea-b81c-1603459431f9"
   },
   "source": [
    "To enable runtime search, we index text chunks by embedding each document split and storing these embeddings in a vector database. Later to search, we embed the query and perform a similarity search to find the stored splits with embeddings most similar to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25619cae",
   "metadata": {},
   "source": [
    "Then, we can read the previously processed and saved vector store back for use;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de3e07d-5fbe-4fe7-8f23-ed0b082f2413",
   "metadata": {
    "id": "5de3e07d-5fbe-4fe7-8f23-ed0b082f2413"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Load the FAISS vectorestore back.\n",
    "VECTOR_STORE = './data/nv_embedding'\n",
    "store = FAISS.load_local(VECTOR_STORE, embedder, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dbd352",
   "metadata": {},
   "source": [
    "#### b) Store all our data into a vectordb (connects vector stores to structured db systems)\n",
    "\n",
    "We will be using **Milvus DB** since it is an open-source, distributed vector DB designed for high-performance vector similarity search across massive datasets (which is what we kind of will have), and makes use of popular libraries like FAISS or Annoy for its vector searching.\n",
    "\n",
    "We have the option to use;\n",
    "1) MilvusVectorStore (Milvus Lite) which is easier to implement and can be more easily tied with ML frameworks --> Milvus-lite does not have a windows version (only supports linux and mac)\n",
    "2) MilvusClient (Python SDK) WITHOUT Docker which was used in a demo to build a RAG system --> Milvus-lite does not have a windows version (only supports linux and mac)\n",
    "3) Direct DB WITH Docker (Milvus Standalone) to directly manage and control the connections as well as increase flexibility --> What im using first since it is the most straighforward (im trying to find if milvus standalone can be used locally instead of through docker)\n",
    "\n",
    "Sources used:\n",
    "- https://milvus.io/docs/quickstart.md\n",
    "- https://milvus.io/docs/integrate_with_langchain.md\n",
    "- https://milvus.io/docs/multimodal_rag_with_milvus.md\n",
    "- https://github.com/milvus-io/bootcamp/tree/master/bootcamp/tutorials/quickstart/apps/multimodal_rag_with_milvus (Possible demo to use)\n",
    "\n",
    "##### Note:\n",
    "Previously, I thought of using **Weaviate** because it is a free, open-source, scalable and reliable vector database service with decent amounts of documentation online.\n",
    "\n",
    "The Weaviate DB works more with Object-Oriented design rather than rows/columns and does not need to be structured like an SQL DB since it already has integrated models/pre-computed embeddings to handle vector embeddings.\n",
    "\n",
    "However, I realised it **could not** be GPU accelerated during its vector retrieval/indexing.\n",
    "\n",
    "Sources used:\n",
    "- https://python.langchain.com/docs/integrations/vectorstores/weaviate/\n",
    "- https://weaviate.io/developers/weaviate/client-libraries/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52344def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-milvus\n",
    "# !pip install -U pymilvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient, DataType, CollectionSchema, FieldSchema\n",
    "import hashlib\n",
    "\n",
    "class MilvusDB:\n",
    "    \"\"\"\n",
    "    Not-so-masterfully handles the vector database using Milvus and all DB related functionality.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, uri):\n",
    "        self.uri = uri\n",
    "        self.client = None\n",
    "        self.collection_name = None\n",
    "\n",
    "        self.similarity_threshold = 0.99  # Similarity between new input data and existing data\n",
    "        self.set_up_db()\n",
    "\n",
    "    def set_up_db(self):\n",
    "        \"\"\"\n",
    "        Starts up connection to Weaviate DB and creates schema if not already created\n",
    "\n",
    "        Parameters: None\n",
    "\n",
    "        Output:     None\n",
    "        \"\"\"\n",
    "        self.client = MilvusClient(uri=self.uri)\n",
    "\n",
    "    def load_collection(self):\n",
    "        \"\"\"\n",
    "        Loads collection into RAM for faster retrieval\n",
    "\n",
    "        Parameters: None\n",
    "\n",
    "        Output:     None\n",
    "        \"\"\"\n",
    "        self.client.load_collection(\n",
    "            collection_name=self.collection_name,\n",
    "            replica_number=1 # Number of replicas to create on query nodes. Max value is 1 for Milvus Standalone, and no greater than `queryNode.replicas` for Milvus Cluster.\n",
    "        )\n",
    "\n",
    "    def release_collection(self):\n",
    "        \"\"\"\n",
    "        Releases the collection from memory to save memory usage\n",
    "\n",
    "        Parameters: None\n",
    "\n",
    "        Output:     None\n",
    "        \"\"\"\n",
    "        self.client.release_collection(\n",
    "            collection_name=self.collection_name\n",
    "        )\n",
    "\n",
    "    def create_collection(self, collection_name, dimensions):\n",
    "        \"\"\"\n",
    "        Creates a new collection in the DB\n",
    "\n",
    "        Parameters: \n",
    "            - collection_name:  Name of collection to make\n",
    "            - dimensions:       Number of dimensions for vector data\n",
    "\n",
    "        Output:     None\n",
    "        \"\"\"     \n",
    "        # Defines a schema to follow when creating the DB\n",
    "        id_field = FieldSchema(name=\"id\", dtype=DataType.VARCHAR, max_length=512, is_primary=True)\n",
    "        embedding_field = FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=dimensions)\n",
    "        text_field = FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=2500)  # Use VARCHAR for string types\n",
    "\n",
    "        # Metadata\n",
    "        source_field = FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=512)  \n",
    "        date_added_field = FieldSchema(name=\"date_added\", dtype=DataType.VARCHAR, max_length=50)\n",
    "        category_field = FieldSchema(name=\"category\", dtype=DataType.VARCHAR, max_length=100)\n",
    "        relevancy_field = FieldSchema(name=\"relevancy\", dtype=DataType.FLOAT)\n",
    "\n",
    "        schema = CollectionSchema(fields=[id_field, embedding_field, text_field, source_field, date_added_field, category_field, relevancy_field])\n",
    "\n",
    "        # Creates the collection\n",
    "        self.client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            dimension=dimensions,\n",
    "            schema=schema,\n",
    "            metric_type=\"COSINE\",             \n",
    "            consistency_level=\"Strong\",     \n",
    "        )\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "        # Creates an index for more efficient similarity search later on based on the metric_type and index_type\n",
    "        self.index_params = MilvusClient.prepare_index_params()\n",
    "        self.index_params.add_index(\n",
    "            field_name=\"embedding\",\n",
    "            metric_type=\"COSINE\",\n",
    "            index_type=\"IVF_FLAT\",\n",
    "            index_name=\"embedding_index\",\n",
    "            params={ \"nlist\": 128 }\n",
    "        )\n",
    "        self.client.create_index(\n",
    "            collection_name=self.collection_name,\n",
    "            index_params=self.index_params,\n",
    "            sync=False\n",
    "        )\n",
    "\n",
    "    def insert_data(self, original, embedded, metadata=None, batch_size=128):\n",
    "        \"\"\"\n",
    "        Adds document and embedding object pairs to the DB collection if not alreadt inside \n",
    "\n",
    "        Parameters:\n",
    "            - original:     Original documents\n",
    "            - embedded:     Embedded documents\n",
    "            - metadata:     List of dictionaries containing metadata for each document\n",
    "            - batch_size:   Number of records per batch\n",
    "\n",
    "        Output: None\n",
    "        \"\"\"\n",
    "        self.load_collection()\n",
    "        data_batch = []\n",
    "\n",
    "        for i, embedded_line in enumerate(tqdm(embedded, desc=\"Inserting data...\")):\n",
    "            unique_id = self.generate_unique_id(embedded_line)            \n",
    "            if self.check_for_similar_vectors(embedded_line) and self.check_for_similar_ids(unique_id):  # Update existing document if ID is found\n",
    "                self.update_existing_document(unique_id, embedded_line, original[i])\n",
    "            else:\n",
    "                categories = self.generate_data_categories(original[i])\n",
    "                data_batch.append(\n",
    "                    {\n",
    "                        \"id\": unique_id, \n",
    "                        \"embedding\": embedded_line, \n",
    "                        \"text\": original[i], \n",
    "                        \"source\": metadata[i].get(\"source\", \"unknown\") if metadata else 'None',\n",
    "                        \"date_added\": metadata[i].get(\"date_added\", \"unknown\") if metadata else 'None',\n",
    "                        \"category\": categories,\n",
    "                        \"relevancy\": 1.0\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            if len(data_batch) >= batch_size:  # Inserts data batch when exceeding max batch size\n",
    "                self.client.insert(collection_name=self.collection_name, data=data_batch)\n",
    "                data_batch = []\n",
    "\n",
    "        if data_batch:\n",
    "            self.client.insert(collection_name=self.collection_name, data=data_batch)\n",
    "\n",
    "        self.release_collection()\n",
    "\n",
    "    def update_existing_document(self, doc_id, embedding, text, relevancy=1.0):\n",
    "        \"\"\"\n",
    "        Updates an existing document in the collection by replacing its embedding and text\n",
    "\n",
    "        Parameters:\n",
    "            - doc_id:       Unique ID of the document\n",
    "            - embedding:    New embedding vector for the document\n",
    "            - text:         New text for the document\n",
    "\n",
    "        Output: None\n",
    "        \"\"\"\n",
    "        self.client.delete(collection_name=self.collection_name, filter=f\"id == '{doc_id}'\")\n",
    "        \n",
    "        updated_data = [{\"id\": doc_id, \"embedding\": embedding, \"text\": text, \"source\": \"None\", \"date_added\":'None', \"category\":\"None\", \"relevancy\":relevancy}]\n",
    "        self.client.insert(collection_name=self.collection_name, data=updated_data)\n",
    "\n",
    "    def update_document_scores(self, doc_id, feedback):\n",
    "        \"\"\"\n",
    "        Updates the relevancy score of certain pieces of data based on feedback from users\n",
    "\n",
    "        Parameters:\n",
    "            - doc_id:       Unique ID of the document\n",
    "            - feedback:     Feedback given by user\n",
    "\n",
    "        Output: None\n",
    "        \"\"\"\n",
    "        print(self.client.search(collection_name=self.collection_name, filter=f\"id == '{doc_id}'\", output_fields=[\"text\", \"source\", \"date_added\", \"category\", \"relevancy\"])[0])\n",
    "        embeddding, text, score = self.client.search(collection_name=self.collection_name, filter=f\"id == '{doc_id}'\", output_fields=[\"text\", \"source\", \"date_added\", \"category\", \"relevancy\"])[0]\n",
    "        \n",
    "        # Update score based on feedback\n",
    "        if feedback == \"Very Relevant\": score += 0.2\n",
    "        elif feedback == \"Not Relevant\": score -= 0.1 \n",
    "        else: score += 0.1\n",
    "\n",
    "        self.update_existing_document(doc_id, embeddding, text, score)\n",
    "\n",
    "    def generate_data_categories(self, text):\n",
    "        \"\"\"\n",
    "        Generates tags associated with a piece of text of question\n",
    "\n",
    "        Parameters:\n",
    "            - data:         Text data to categorise\n",
    "\n",
    "        Returns:\n",
    "            - categories:   List of categories relevant to data\n",
    "        \"\"\"\n",
    "        categories = llm.invoke(f\" \\\n",
    "            You are an expert in Singapore's corporate laws and regulations. Your task is to analyze the given text or question, and assign it appropriate tags from the list below. Tags should capture the core topics, entities, and relevant categories mentioned in the text or question given. \\\n",
    "            Only return 3 words of the the top 3 most relevant tags from the list below, without giving any other comments. \\\n",
    "            List of Tags: \\\n",
    "            - Sole Proprietorship, Partnership, Company, Corporation \\\n",
    "            - Business Registration, Taxation, Licensing, Employment Laws, Compliance, Intellectual Property, Contracts \\\n",
    "            - Setup, Operations, Reporting, Closure \\\n",
    "            - Healthcare, Retail, Finance, Technology, Food \\\n",
    "            - Forms, Policies, Guidelines, FAQs \\\n",
    "            - Compliance Issues, Tax Problems, Employment Disputes, Licensing Delays \\\n",
    "            - Singapore, International Trade \\\n",
    "            Input Text: {text} \\\n",
    "            Output Format: Comma-separated list of tags \\\n",
    "        \").content.strip().lower()\n",
    "        categories = min(categories[0:len(categories)],  categories[0:100])\n",
    "        print(categories, type(categories))\n",
    "        return categories\n",
    "\n",
    "    def generate_unique_id(self, data):\n",
    "        \"\"\"\n",
    "        Generates a unique hash ID based on the vector or text data\n",
    "\n",
    "        Parameters:\n",
    "            - data: The vector or text data used to generate the hash\n",
    "\n",
    "        Returns:\n",
    "            - id:   A unique hash ID as a string\n",
    "        \"\"\"\n",
    "        data_str = str(data)\n",
    "        unique_id = hashlib.sha256(data_str.encode()).hexdigest()\n",
    "        return unique_id\n",
    "\n",
    "    def check_for_similar_vectors(self, embedding, top_k=5):\n",
    "        \"\"\"\n",
    "        Checks the DB for vectors that are similar to the input embedding (based on distance metric like cosine similarity or Euclidean distance)\n",
    "\n",
    "        Parameters:\n",
    "            - embedding:    Embedded documents\n",
    "            - top_k:        Top K number of documents that are similar to the input embedding\n",
    "\n",
    "        Output: \n",
    "            - check:        True or False value of whether a similar vector has been found\n",
    "        \"\"\"\n",
    "        try:\n",
    "            search_params = {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}}\n",
    "            results = self.client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                data=[embedding],\n",
    "                anns_field=\"embedding\",  # Adjust field name based on your Milvus schema\n",
    "                search_params=search_params,\n",
    "                limit=top_k\n",
    "            )\n",
    "            for result in results:\n",
    "                for vector in result:\n",
    "                    if vector['distance'] >= self.similarity_threshold:\n",
    "                        return True  \n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking if vector exists: {e}\")\n",
    "            return False\n",
    "\n",
    "    def check_for_similar_ids(self, id):\n",
    "        \"\"\"\n",
    "        Checks the DB for ids that are similar to the new one created\n",
    "\n",
    "        Parameters:\n",
    "            - id:       Unique id generated for new row\n",
    "\n",
    "        Output: \n",
    "            - check:    True or False value of whether a similar vector has been found\n",
    "        \"\"\"\n",
    "        try:\n",
    "            results = self.client.query(\n",
    "                collection_name=self.collection_name,\n",
    "                filter=f\"id == '{id}'\",  \n",
    "                output_fields=[\"id\"], limit=1000 \n",
    "            )\n",
    "            return True if results else False\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking if ID exists: {e}\")\n",
    "            return False\n",
    "\n",
    "    def retrieve_data(self, embedded_question, question, top_k=5, scaling_factor=1.1):\n",
    "        \"\"\"\n",
    "        Retrieves vector data from DB based on embedded question\n",
    "\n",
    "        Parameters:\n",
    "            - question:             Question as a string\n",
    "            - embedded_question:    Embedded question as a vector\n",
    "\n",
    "        Output:\n",
    "            - search_res:           Results of vector retrieval\n",
    "        \"\"\"\n",
    "        self.load_collection()\n",
    "\n",
    "        category_filter = \" or \".join([f'category like \"%{category}%\"' for category in self.generate_data_categories(question).replace(' ', '').split(',')])\n",
    "\n",
    "        search_res = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            data=[embedded_question],  \n",
    "            limit=top_k, \n",
    "            search_params={\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}},\n",
    "            filter=category_filter, \n",
    "            output_fields=[\"text\", \"source\", \"date_added\", \"category\"]\n",
    "        )[0]\n",
    "\n",
    "        distances = [res['distance'] for res in search_res]\n",
    "        avg_distance = sum(distances) / len(distances) if distances else 0\n",
    "        adaptive_threshold = avg_distance * scaling_factor  # Getting a dynamic threshold based on average distance of top-K results to improve relevance filtering\n",
    "\n",
    "        search_res = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            data=[embedded_question],  \n",
    "            limit=20, \n",
    "            search_params={\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}},\n",
    "            filter=category_filter, \n",
    "            output_fields=[\"text\", \"source\", \"date_added\", \"category\", \"relevancy\"]\n",
    "        )[0]\n",
    "\n",
    "        filtered_res = [res for res in search_res if res['distance'] > adaptive_threshold]\n",
    "        self.release_collection()\n",
    "        return filtered_res\n",
    "    \n",
    "    def get_all_records(self, limit=10000):\n",
    "        \"\"\"\n",
    "        Retrieves all rows from the Milvus collection.\n",
    "        \n",
    "        Parameters:\n",
    "            - limit: The maximum number of rows to fetch in one query. Adjust as needed.\n",
    "        \n",
    "        Returns:\n",
    "            - List of all records in the collection.\n",
    "        \"\"\"\n",
    "        self.load_collection()\n",
    "\n",
    "        try:\n",
    "            # Query all records\n",
    "            all_records = self.client.query(\n",
    "                collection_name=self.collection_name,\n",
    "                output_fields=[\"id\", \"text\", \"source\", \"date_added\", \"category\"],\n",
    "                limit=limit  # Adjust limit as needed\n",
    "            )\n",
    "            return all_records\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving records: {e}\")\n",
    "            return []\n",
    "        finally:\n",
    "            self.release_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8419498",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = MilvusDB('http://localhost:19530')\n",
    "database.collection_name = \"Documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89671eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting data...:  14%|█▍        | 1/7 [00:01<00:07,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company, licensing, compliance, singapore, workplace safety and health act, inspection company, veri <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting data...:  29%|██▊       | 2/7 [00:02<00:05,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company, operations, compliance <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting data...:  43%|████▎     | 3/7 [00:03<00:04,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company, operations, compliance <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting data...:  57%|█████▋    | 4/7 [00:03<00:02,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company, operations, compliance, healthcare, faqs, policies, safety, maintenance <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting data...:  71%|███████▏  | 5/7 [00:04<00:01,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company, operations, compliance, workplace safety and health act, recommended follow-up actions <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting data...:  86%|████████▌ | 6/7 [00:05<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxation, singapore, corporate income tax <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting data...: 100%|██████████| 7/7 [00:06<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company, taxation, schemes/programs/initiatives\n",
      "\n",
      "(the text covers various taxation schemes and other <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "database = MilvusDB('http://localhost:19530')\n",
    "database.create_collection(\"Documents\", 1024)\n",
    "database.insert_data(datahandler.textual_data + datahandler.tabular_data, datahandler.embedded_data, datahandler.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5156df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "database.client.drop_collection(\"Documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91789291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'text': 'Home Schemes Schemes Government Schemes Learn about Government Schemes which may be applicable for you. Self-review for Eligibility of Payouts Progressive Wage Credit Scheme (PWCS) Uplifting Employment Credit (UEC) Enterprise Innovation Scheme (EIS) Jobs Growth Incentive (JGI) Wage Credit Scheme (WCS) Jobs Support Scheme (JSS) Senior Employment Credit (SEC), CPF Transition Offset (CTO) and Enabling Employment Credit (EEC) SkillsFuture Enterprise Credit MediShield Life (MSHL) Refundable Investment Credit (RIC) Corporate Income Tax Learn about Corporate Income Tax Schemes which may be applicable for you. Double Tax Deduction for Internationalisation Scheme Productivity and Innovation Credit (PIC) Scheme Corporate Volunteer Scheme GST Learn about GST schemes which may be applicable for you. Industry-Specific GST Schemes General GST Schemes Individual Income Tax Learn about Individual Income Tax schemes which may be applicable for you.', 'source': 'https://www.iras.gov.sg/schemes', 'date_added': 'None', 'category': 'company, taxation, schemes/programs/initiatives\\n\\n(the text covers various taxation schemes and other', 'id': '3017d523e449518d1f495d905f972a50c1012ce6c58e3bc63af71f7d4ff1b705'}\n",
      "{'text': 'Home Taxes Taxes Individual Income Tax Learn more about individual income tax. Corporate Income Tax Learn more about corporate income tax. Property Tax Learn more about property tax. Goods & Services Tax (GST) Learn more about GST. Stamp Duty Learn more about stamp duty. International Tax Learn more about international tax. Withholding Tax Learn more about withholding tax. Other Taxes Learn more about other taxes.', 'source': 'https://www.iras.gov.sg/taxes', 'date_added': 'None', 'category': 'taxation, singapore, corporate income tax', 'id': '68fd5b0d756512dd8cb67127c82964e8c0d2afc989ebb9d4cb9a736d90e415e5'}\n",
      "{'text': 'that the specified machine is safe for its intended use in a work environment and can be operated safely, across the various life cycle stages: 1. Acquisition 2. Installation 3. Testing and commissioning 4. Operation 5. Maintenance and repairs 2.1 Acquisition of machine (new or used) S/N Items to Check Y N NA1 Discussion of Evidence/ Comments Safe when properly used 2.1.1 a) Is the machine designed and built to applicable guidelines and standards? - Discuss how the criteria is met with references to guidelines and standards. b) Is the machine tested and examined according to applicable guidelines and standards? - Discuss how the criteria is met with references to guidelines and standards. Suitable and safe for its intended use 2.1.2 a) Is the machine suitable for its intended use? - Discuss about the design use and intended use. b) Is there a manual provided with the machine? i) In a language understood by the relevant personnel? 1 Y – Yes, N – No, NA – Not applicable. Provide comments for all ‘NA’. S/N Items to Check Y N NA1 Discussion of Evidence/ Comments ii) Are the instructions within comprehensive and clear? c) Is technical support for the machine available? - Describe what is available. Suitable and safe for use in the intended environment 2.1.3 a) Is there information on the recommended operating environment or limits on the operating environment for the machine (e.g. dust, temperature, etc.)? - Discuss about environment the machine is or not intended to be used in. b) Is the machine suitable to be used in the intended work environment? Additional checks for used machine 2.1.4 a) Is there proper documentation of the machine’s operation, maintenance and repair history? b) If there were modifications or retrofitting done to the machine, were these done according to information supplied by the designer, manufacturer or supplier? 2.2 Installation S/N Items to Check Y N NA Discussion of Evidence/ Comments Adequate information for safe installation 2.2.1 a) Is there information on what needs to be prepared or', 'source': './data/pdfs\\\\verification-checklist-inspection-of-machines.pdf', 'date_added': 'None', 'category': 'company, operations, compliance', 'id': '6f808b849c073199599e630245d43680c815da31178028c4353816eb37715102'}\n",
      "{'text': 'a) Is there information on personnel competency (e.g. training, verification) required for maintenance and repair of the machine? b) Are competency requirements for personnel carrying out maintenance and repair of the machine met? c) Are specialised tools required to carry out the maintenance and repair of the machine available? 3. Discussion [This section shall discuss the findings and assessment by the inspection company such as if any response in the checklist is “No”, the inspection company must discuss the implication and make an assessment whether the machine is safe for operation.] 4. Conclusion [This section shall conclude if the machine inspected is verified to conform to requirements in the Singapore Standard SS 537: Code of Practice for Safe Use of Machinery – Part 1: General Requirements and relevant Workplace Safety and Health Act provisions, or unacceptable for safe use, with recommended follow-up actions in Annex A.] Inspector Name and Signature Approved Signatory’s Name and Signature Inspection Company’s Name DD/MM/YYYY Date of Verification Annex A: Recommended Follow-up Actions S/No Item Description Recommended Follow-up Actions Annex B: Examples of hazards a. sharp edges/corners and protruding parts b. crush, shear, and entanglement points c. contact or entanglement with the machinery d. trapping between the machine and any material or fixed structures e. ergonomic hazards f. being struck by ejected parts of the machinery g. being struck by material ejected from the machinery h. electrical shock i. chemicals that are toxic, irritant, flammable, corrosive, explosive j. fire and explosion k. combustible dust l. noise and vibration m. temperature (high, low) n. inhalation of mist, fume and dust o. overpressure', 'source': './data/pdfs\\\\verification-checklist-inspection-of-machines.pdf', 'date_added': 'None', 'category': 'company, operations, compliance, workplace safety and health act, recommended follow-up actions', 'id': 'cb9fb4211926c22c943f14a1f02e6a9ec2f535873dd697f001e7dabbe7e47524'}\n",
      "{'text': 'Items to Check Y N NA Discussion of Evidence/ Comments Adequate information for the safe operation 2.4.1 a) Is there information on what needs to be prepared or completed before operating the machine? - Describe what these are. b) Is the machine provided with instructions for safe operations? c) Is there information about safety features, safety devices or warnings/precautions? - Describe what these are. d) Do the safety features and safety devices function as intended? S/N Items to Check Y N NA Discussion of Evidence/ Comments e) Is there information on the safe operating limits of the machine? - Describe what these are. f) Are there unusual hazards or complicated machine features, and have they been identified? - Describe what these are. g) Is there information about residual risks posed by the machine? - Describe what these are. h) Are there troubleshooting instructions provided for the machine? i) Is there information on emergency response procedures? Hazards/risks have been identified and mitigated 2.4.2 a) Have the hazards associated with operating the machine identified and control measures established? List what they are and how they are mitigated: Hazards (see Annex B for examples) Discussion on Adequacy - List hazards - Discuss whether risks are adequately mitigated 2.5 Maintenance and Repairs S/N Items to Check Y N NA Discussion of Evidence/ Comments Adequate information for maintenance and repairs 2.5.1 a) Is there information on maintenance and repair requirements for the machine? S/N Items to Check Y N NA Discussion of Evidence/ Comments b) Is the machine provided with instructions on its maintenance and repair? c) Is there information on the hazards relating to the maintenance and repair of the machine? - Describe what these are. d) Is there information on the precautions to be taken during maintenance and repair of the machine (eg. LOTO)? - Describe what these are. Adequate and suitable resources for the maintenance and repairs 2.5.2 a) Is there information on personnel competency (e.g. training,', 'source': './data/pdfs\\\\verification-checklist-inspection-of-machines.pdf', 'date_added': 'None', 'category': 'company, operations, compliance, healthcare, faqs, policies, safety, maintenance', 'id': 'd1a97c30fe3807149621b3180959e1036f70fcb91a928eadb592cdc1218ae609'}\n",
      "{'text': 'INSPECTION PROGRAMME FOR SAFE MACHINES Verification Checklist Objective This checklist shall be used as a verification guide that a machine in a workplace is safe for use at various life stages. About this Verification Checklist This checklist is based on the Singapore Standard SS 537: Code of Practice for Safe Use of Machinery – Part 1: General Requirements and relevant provisions under the Workplace Safety and Health Act (WSH Act). It contains basic verification items and additional checks where relevant. Each individual machine must be accompanied by its own completed checklist. For a machine to be verified safe for use, all responses in the checklist must be “Yes” or “Not Applicable”, and if any response is “No”, the inspection company must discuss the implication and make an assessment whether the machine is safe for operation. Verification by an Inspection Company The verification shall be conducted by any Inspection Company listed on the Ministry of Manpower’s website, with the outcome documented in a report prepared by the Inspection Company. The Inspection Company shall in its report cover all the items in this checklist and provide objective explanations and evidence for each item verified (document reviewed, equipment inspected, etc.). The report shall conclude with either: - “Verified to conform to requirements in the Singapore Standard SS 537: Code of Practice for Safe Use of Machinery – Part 1: General Requirements and relevant Workplace Safety and Health Act provisions”, or - “Unacceptable for safe use” with recommended follow up actions. 1. Details of Machine a) Owner - Name - UEN - Address - Contact Details b) Manufacturer/Supplier - Name - Address - Contact Details c) Brand/ Model - Brand - Model d) Description of Machine - General description of physical appearance - Key components and functions/applications - Condition (New, Used, Refurbished, Operation history) - Date procured/acquired 2. Verification Scope This checklist ensures that the specified machine is safe for its intended use in a', 'source': './data/pdfs\\\\verification-checklist-inspection-of-machines.pdf', 'date_added': 'None', 'category': 'company, licensing, compliance, singapore, workplace safety and health act, inspection company, veri', 'id': 'd45da29f7eee7dae776672b7ff654e5cc1e37c8bbebd2331c96adf04ab6cdf9e'}\n",
      "{'text': '2.2.1 a) Is there information on what needs to be prepared or done before the machine is delivered to the site? - Describe what these are. b) Does the machine come with installation instructions? S/N Items to Check Y N NA Discussion of Evidence/ Comments c) Is there information on the hazards relating to the installation of the machine? - Describe what these are. d) Is there information on the precautions to take during installation? - Describe what these are. Adequate and suitable resources for the safe installation 2.2.2 a) Is there information on personnel competency (e.g. training, licences) required for installation of the machine? b) Are competency requirements for personnel carrying out installation of the machine met? c) Are specialised tools required for installation of the machine available? 2.3 Testing and Commissioning S/N Items to Check Y N NA Discussion of Evidence/ Comments Adequate information for safe testing and commissioning 2.3.1 a) Is there information on what needs to be prepared or done before testing and commissioning the machine? - Describe what these are. b) Is the machine provided with testing and commissioning procedures? (e.g. tests to be conducted, checklists, etc.) c) Is there information on the hazards relating to the testing and commissioning of the machine? - Describe what these are. S/N Items to Check Y N NA Discussion of Evidence/ Comments d) Is there information on the precautions to take during testing and commissioning? - Describe what these are. e) Is the machine configured for the intended use? Adequate and suitable resources for the safe testing and commission 2.3.2 a) Is there information on personnel competency (e.g. training, licences) required for testing and commissioning of the machine? b) Are competency requirements for personnel carrying out testing and commissioning of the machine met? c) Are specialised tools required for testing and commissioning of the machine available? 2.4 Operation S/N Items to Check Y N NA Discussion of Evidence/ Comments Adequate', 'source': './data/pdfs\\\\verification-checklist-inspection-of-machines.pdf', 'date_added': 'None', 'category': 'company, operations, compliance', 'id': 'e83cca6ecf66c00891d171e739d1d5091363405b5fd92dc4c600f8bc345791c3'}\n"
     ]
    }
   ],
   "source": [
    "records = database.get_all_records()\n",
    "print(len(records))\n",
    "for record in records: print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a41ff63-6adc-4055-8bc4-e7ecaad0fb4d",
   "metadata": {
    "id": "4a41ff63-6adc-4055-8bc4-e7ecaad0fb4d"
   },
   "source": [
    "### 5) Using data to answer questions\n",
    "\n",
    "With our stored embedded data, we can retrieve relevant vectors stored in our vectorstore/db to answer embedded questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9ce3ba",
   "metadata": {},
   "source": [
    "#### A) Using vectorstore to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa362c9-48ab-4646-bc29-bc2aca92505d",
   "metadata": {
    "id": "5aa362c9-48ab-4646-bc29-bc2aca92505d"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "context = store.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer solely based on the following:\\n<Documents>\\n{context}\\n</Documents>\",\n",
    "        ),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Langchain's LCEL(LangChain Expression Language) Runnable protocol is used to define the chain\n",
    "# LCEL allows pipe together components and functions\n",
    "chain = (\n",
    "    {\"context\": context, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a0191d",
   "metadata": {},
   "source": [
    "#### B) Using VectorDB to answer\n",
    "\n",
    "What happens if our vectordb does not have the relevant information needed though?\n",
    "\n",
    "#### C) Using web sources to answer\n",
    "\n",
    "Using Tavily, we can create a search engine that calls abstract searching, scraping, filtering and extracting from online sources.\n",
    "\n",
    "Only issue is, it takes too long to get the answer out, so it should only be used as a final resort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3048726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd9c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, MessagesState\n",
    "from typing import Literal\n",
    "\n",
    "class SearchEngine:\n",
    "    def __init__(self):\n",
    "        self.search_tool = TavilySearchResults(max_results=3)\n",
    "        self.llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "\n",
    "        self.tools = [self.search_tool]  # Add other tools here if needed\n",
    "        self.llm_with_tools = self.llm.bind_tools(self.tools)\n",
    "        self.workflow = self.build_workflow()\n",
    "\n",
    "    def build_workflow(self):\n",
    "        \"\"\"Builds the state graph workflow.\"\"\"\n",
    "        workflow = StateGraph(MessagesState)\n",
    "        \n",
    "        # Add nodes for agent (LLM) and tools (search)\n",
    "        workflow.add_node(\"agent\", self.call_model)\n",
    "        workflow.add_node(\"tools\", ToolNode(self.tools))\n",
    "        \n",
    "        # Add conditional logic to determine if tools should be used\n",
    "        workflow.add_edge(\"__start__\", \"agent\")\n",
    "        workflow.add_conditional_edges(\"agent\", self.should_continue)\n",
    "        workflow.add_edge(\"tools\", \"agent\")\n",
    "        \n",
    "        return workflow.compile()\n",
    "\n",
    "    def call_model(self, state):\n",
    "        \"\"\"Invoke the LLM with the current state.\"\"\"\n",
    "        messages = state[\"messages\"]\n",
    "        response = self.llm_with_tools.invoke(messages)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    def should_continue(self, state):\n",
    "        \"\"\"Determine whether to continue invoking tools or end the workflow.\"\"\"\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        if last_message.tool_calls:\n",
    "            return \"tools\"\n",
    "        return \"__end__\"\n",
    "\n",
    "    def query(self, user_query, context=' in singapore'):\n",
    "        \"\"\"Main method to process the user's query.\"\"\"\n",
    "        initial_state = MessagesState({\"messages\": [self.llm_with_tools.invoke(user_query + context)]})\n",
    "        return self.workflow.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198905d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question):\n",
    "    relevance = llm.invoke(f\" \\\n",
    "        Context from database:\\n\\n\\n\\\n",
    "        User's question: {question}\\n\\n\\\n",
    "        Is the question relevant to navigating Legal and Regulatory Requirements in Singapore's Corporate Landscape, involving taxes, employment, costs and such? \\\n",
    "        Reply only either a 'yes' or 'no'.\\\n",
    "    \").content.strip().lower()\n",
    "    \n",
    "    if 'yes' in relevance:\n",
    "        retrieved_data = database.retrieve_data(embedder.embed_query(question), question)\n",
    "\n",
    "        if len(retrieved_data) > 0:\n",
    "            retrieved_lines_with_distances = [\n",
    "                (res[\"entity\"][\"text\"], res[\"distance\"], res[\"entity\"][\"source\"]) for res in retrieved_data\n",
    "            ]\n",
    "            context = '\\n'.join(\n",
    "                [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances]\n",
    "            )\n",
    "            prompt = f\" \\\n",
    "                Context from database:\\n{context}\\n\\n\\\n",
    "                User's question: {question}\\n\\n\\\n",
    "                Please generate an informative response and cite the source after.\\\n",
    "                If the question is irrelevant to SME Regulations, reply that that the question is irrelevant.\\\n",
    "            \"\n",
    "            return llm.invoke(prompt).content\n",
    "        \n",
    "        print(\"No data found in database. Searching online...\")\n",
    "        search_engine = SearchEngine()\n",
    "        response = search_engine.query(question)\n",
    "        print(response)\n",
    "        for message in response['messages']:\n",
    "            print(message.content)\n",
    "            database.insert_data(message.content, embedder.embed_query(message.content))\n",
    "        return\n",
    "    else:\n",
    "        return \"The question is irrelevant to SME Regulations.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08866cc3",
   "metadata": {},
   "source": [
    "#### Now, we can try asking different types of questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d318b0",
   "metadata": {
    "id": "18d318b0"
   },
   "source": [
    "Case 1: Irrelevant question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9798ab63",
   "metadata": {
    "id": "9798ab63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no information provided in the documents about a component or entity called \"two hundred and twenty five million.\" The documents contain information about various financial transactions, mortgage and loan balances, estate duty calculations, and rules related to M&A allowance and motor vehicle expenses, but there is no mention of a \"component\" with a value of 225 million.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"What is component two hundred and twenty five million?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f474d0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question is irrelevant to SME Regulations.\n"
     ]
    }
   ],
   "source": [
    "print(ask(\"What is component two hundred and twenty five million?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3223cefb",
   "metadata": {
    "id": "3223cefb"
   },
   "source": [
    "Case 2: Simple questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ba4ef8",
   "metadata": {
    "id": "28ba4ef8"
   },
   "outputs": [],
   "source": [
    "print(chain.invoke(\"How do i file taxes for my company?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9a008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company, taxation, reporting, singapore <class 'str'>\n",
      "category like \"%company%\" or category like \"%taxation%\" or category like \"%reporting%\" or category like \"%singapore%\"\n",
      "[]\n",
      "No data found in database. Searching online...\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for TavilySearchAPIWrapper\n__root__\n  Did not find tavily_api_key, please add an environment variable `TAVILY_API_KEY` which contains it, or pass `tavily_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[143], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow do i file taxes for my company?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[142], line 28\u001b[0m, in \u001b[0;36mask\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\u001b[38;5;241m.\u001b[39minvoke(prompt)\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data found in database. Searching online...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m search_engine \u001b[38;5;241m=\u001b[39m \u001b[43mSearchEngine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m response \u001b[38;5;241m=\u001b[39m search_engine\u001b[38;5;241m.\u001b[39mquery(question)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[99], line 9\u001b[0m, in \u001b[0;36mSearchEngine.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_tool \u001b[38;5;241m=\u001b[39m \u001b[43mTavilySearchResults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m ChatNVIDIA(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta/llama-3.1-8b-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_tool]  \u001b[38;5;66;03m# Add other tools here if needed\u001b[39;00m\n",
      "File \u001b[1;32mc:\\GERARD\\Coding\\RAG\\project\\venv\\Lib\\site-packages\\langchain_core\\tools\\base.py:346\u001b[0m, in \u001b[0;36mBaseTool.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_basemodel_subclass(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs_schema\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    343\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs_schema must be a subclass of pydantic BaseModel. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    344\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs_schema\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    345\u001b[0m         )\n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\GERARD\\Coding\\RAG\\project\\venv\\Lib\\site-packages\\langchain_core\\load\\serializable.py:113\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\GERARD\\Coding\\RAG\\project\\venv\\Lib\\site-packages\\pydantic\\v1\\main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[1;32mc:\\GERARD\\Coding\\RAG\\project\\venv\\Lib\\site-packages\\pydantic\\v1\\main.py:1064\u001b[0m, in \u001b[0;36mvalidate_model\u001b[1;34m(model, input_data, cls)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(ErrorWrapper(MissingError(), loc\u001b[38;5;241m=\u001b[39mfield\u001b[38;5;241m.\u001b[39malias))\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1064\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[43mfield\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mvalidate_all \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m field\u001b[38;5;241m.\u001b[39mvalidate_always:\n\u001b[0;32m   1067\u001b[0m     values[name] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[1;32mc:\\GERARD\\Coding\\RAG\\project\\venv\\Lib\\site-packages\\pydantic\\v1\\fields.py:437\u001b[0m, in \u001b[0;36mModelField.get_default\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_default\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m smart_deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\GERARD\\Coding\\RAG\\project\\venv\\Lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for TavilySearchAPIWrapper\n__root__\n  Did not find tavily_api_key, please add an environment variable `TAVILY_API_KEY` which contains it, or pass `tavily_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "print(ask(\"How do i file taxes for my company?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0f962",
   "metadata": {
    "id": "87b0f962"
   },
   "source": [
    "Case 3: Complex questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c2e9c",
   "metadata": {
    "id": "af3c2e9c"
   },
   "outputs": [],
   "source": [
    "print(chain.invoke(\"In the event my foriegn employee is injured at work, how do i report the incident and claim reparations?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yxJMeKE-LItu",
   "metadata": {
    "id": "yxJMeKE-LItu"
   },
   "source": [
    "Case 4: Realistic questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SM9lTRNlLItw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SM9lTRNlLItw",
    "outputId": "16202adf-df26-453a-f34e-820968f35832"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From 2025 onwards, as an employer whose firm qualifies for PWCS (Productivity Works Credits Scheme), you can receive co-funding from the government of up to 50% for the first tier of wage increases and 15% to 30% for the second tier of wage increases. This co-funding support applies to wage increases given in qualifying year 2025 and onwards. The gross monthly wage ceiling for PWCS co-funding will be increased to $3,000 in qualifying years 2025 and 2026.\\n\\nPlease note that the specific rates and details of the co-funding support may be subject to changes or updates in the PWCS guidelines. It is recommended to consult the official guidelines or contact the relevant authorities for the most accurate and up-to-date information.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"I am an employer whose firm qualifies for PWCs. From 2025 onwards, how much Co-Funding can I recieve from the government?\")\n",
    "# In its current state, the model does an ethan for this quite hard question, although it at least does retrieve some (few?) relevant PWCS info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3854c7-68a3-45b4-9e69-2c4e583d651f",
   "metadata": {
    "id": "9d3854c7-68a3-45b4-9e69-2c4e583d651f"
   },
   "source": [
    "### 6) Enhancing accuracy for single data sources\n",
    "\n",
    "This example demonstrates how a re-ranking model can be used to combine retrieval results and improve accuracy during retrieval of documents.\n",
    "\n",
    "Typically, reranking is a critical piece of high-accuracy, efficient retrieval pipelines. Generally, there are two important use cases:\n",
    "\n",
    "- Combining results from multiple data sources\n",
    "- Enhancing accuracy for single data sources\n",
    "\n",
    "Here, we focus on demonstrating only the second use case. If you want to know more, check [here](https://github.com/langchain-ai/langchain-nvidia/blob/main/libs/ai-endpoints/docs/retrievers/nvidia_rerank.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e8677e-a37f-42e2-8fea-4c4413f7d682",
   "metadata": {
    "collapsed": true,
    "id": "b7e8677e-a37f-42e2-8fea-4c4413f7d682"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIARerank\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# We will narrow the collection to 100 results and further narrow it to 10 with the reranker.\n",
    "retriever = store.as_retriever(search_kwargs={'k':100}) # typically k will be 1000 for real world use-cases\n",
    "ranker = NVIDIARerank(model='nv-rerank-qa-mistral-4b:1', top_n=10)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer solely based on the following context:\\n<Documents>\\n{context}\\n</Documents>\",\n",
    "        ),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "reranker = lambda input: ranker.compress_documents(query=input['question'], documents=input['context'])\n",
    "\n",
    "chain_with_ranker = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | {\"context\": reranker, \"question\": lambda input: input['question']}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1940aae",
   "metadata": {
    "id": "f1940aae",
    "outputId": "33a3f36f-5d7a-41a4-b919-56e724ff9f7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided documents, to file taxes for your company, you need to follow these steps:\n",
      "\n",
      "1. Ensure that you are duly authorized by your company as an 'Approver' for Corporate Tax (Filing and Applications) in Corppass. You can refer to the step-by-step guides for assistance on Corppass setup.\n",
      "2. Have your Singpass and your company’s Unique Entity Number (UEN)/ Entity ID ready.\n",
      "3. Visit the mytax.iras.gov.sg website to file the Corporate Income Tax Return for your company.\n",
      "4. If your company is filing Form C, you need to submit the financial statements/certified accounts and tax computation(s) for the relevant Year of Assessment (YA).\n",
      "5. If your company meets the qualifying conditions to file Form C-S or Form C-S (Lite), you can choose to file the simplified version, Form C-S (Lite), if your company has an annual revenue of $200,000 or below.\n",
      "\n",
      "You can also visit the Basic Guide to Corporate Income Tax for Companies page to get help with filing your company’s tax returns for the first time. Additionally, you can access the New Company Start-Up Kit, an interactive guide, to learn more.\n"
     ]
    }
   ],
   "source": [
    "print(chain_with_ranker.invoke(\"How do i file taxes for my company?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30083cbb",
   "metadata": {
    "id": "30083cbb",
    "outputId": "a517c0c1-21cf-4b66-bf32-0cc299513ea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided documents, if your foreign employee is injured at work, you can report the incident and claim reparations under the Work Injury Compensation Act (WICA). Specifically, the documents mention that input tax can be claimed for work injury compensation insurance that is obligatory under WICA for both local and foreign employees performing manual work or non-manual work earning $2,600 or less a month.\n",
      "\n",
      "To report the incident and make a claim, you can visit the Ministry of Manpower (MOM) webpage on WICA or contact MOM at +65 6438 5122. However, it is important to note that medical and accident insurance premiums for your staff are generally not allowable for input tax claims under the GST (General) Regulations, unless the insurance or payment of compensation is obligatory under WICA or under any collective agreement within the meaning of the Industrial Relations Act.\n",
      "\n",
      "Therefore, it seems that you can report the incident and claim reparations under WICA, but you should verify the specific requirements and conditions for your situation with MOM or your trade union.\n"
     ]
    }
   ],
   "source": [
    "print(chain_with_ranker.invoke(\"In the event my foriegn employee is injured at work, how do i report the incident and claim reparations?\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
