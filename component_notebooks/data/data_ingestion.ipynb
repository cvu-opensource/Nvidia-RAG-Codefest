{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5fb0b9e-f9cd-404f-bd8d-0273e94ac1fe",
   "metadata": {
    "id": "c5fb0b9e-f9cd-404f-bd8d-0273e94ac1fe"
   },
   "source": [
    "# Part 1 pre-acceptance workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2969cdab-82fc-4ce5-bde1-b4f629691f27",
   "metadata": {
    "id": "2969cdab-82fc-4ce5-bde1-b4f629691f27"
   },
   "source": [
    "This notebook is meant to be a prototype for the NVIDIA NIM hackathon project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca300278-5ff4-47c4-ab70-c6584ef73c9f",
   "metadata": {
    "id": "ca300278-5ff4-47c4-ab70-c6584ef73c9f"
   },
   "source": [
    "## Installation and Requirements\n",
    "\n",
    "Shits fucked cunt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de8e53a-7940-4e72-87f3-e1c014128806",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "5de8e53a-7940-4e72-87f3-e1c014128806",
    "outputId": "2f26a4cb-ef97-4590-c9c8-c1118e1f3b65"
   },
   "outputs": [],
   "source": [
    "# Requirements\n",
    "!pip install langchain==0.2.5\n",
    "!pip install langchain_community==0.2.5\n",
    "!pip install faiss-gpu # replace with faiss-gpu if you are using GPU\n",
    "!pip install faiss-cpu\n",
    "!pip install langchain-nvidia-ai-endpoints==0.1.2\n",
    "!pip install requests pdfplumber spacy camelot-py \n",
    "!pip install pandas==2 numpy==1.26.4 \n",
    "!pip install beautifulsoup4 \n",
    "!pip install pymupdf\n",
    "!pip install lxml\n",
    "!pip install unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7a52a0-7e5e-4064-9665-cb947d600f84",
   "metadata": {
    "id": "1b7a52a0-7e5e-4064-9665-cb947d600f84"
   },
   "source": [
    "## Getting Started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04495732-c2db-4c97-91d0-96708814334d",
   "metadata": {
    "id": "04495732-c2db-4c97-91d0-96708814334d"
   },
   "source": [
    "You need an `NVIDIA_API_KEY` to use the NVIDIA API Catalog:\n",
    "\n",
    "1) Create a free account with [NVIDIA](https://build.nvidia.com/explore/discover).\n",
    "2) Click on your model of choice.\n",
    "3) Under Input select the Python tab, and click **Get API Key** and then click **Generate Key**.\n",
    "4) Copy and save the generated key as NVIDIA_API_KEY. From there, you should have access to the endpoints.\n",
    "5) If at any point downstream NVIDIA complains about insufficient credits, tell them to shut the fuck up and make a new gmail. If their stock price is so high, they should share some of those profits with me, an NVIDIA investor (i own 3 shares rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbb51115-79f8-48c3-b3ee-d434916945f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbb51115-79f8-48c3-b3ee-d434916945f6",
    "outputId": "e4d2f493-998c-42ab-8d7e-60c7cffcd318"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "assert nvidia_api_key.startswith(\"nvapi-\"), f\"{nvidia_api_key[:5]}... is not a valid key\"\n",
    "os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25656ab5-0046-4e27-be65-b3d3d547b4c6",
   "metadata": {
    "id": "25656ab5-0046-4e27-be65-b3d3d547b4c6"
   },
   "source": [
    "## LLM & Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e86bc0-e9c5-4a2b-be0e-7fca0331e886",
   "metadata": {
    "id": "54e86bc0-e9c5-4a2b-be0e-7fca0331e886"
   },
   "source": [
    "### 1) Initialize the LLM\n",
    "\n",
    "The ChatNVIDIA class is part of LangChain's integration (langchain_nvidia_ai_endpoints) with NVIDIA NIM microservices.\n",
    "It allows access to NVIDIA NIM for chat applications, connecting to hosted or locally-deployed microservices.\n",
    "\n",
    "Here we will use **mixtral-8x7b-instruct-v0.1**\n",
    "\n",
    "Note: You can use any model hosted at the NVIDIA API catalog using 'ChatNVIDIA.get_available_models()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88c2fafe-5ded-4238-82de-f094232bf6fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "88c2fafe-5ded-4238-82de-f094232bf6fb",
    "outputId": "fa6a7c4a-2ddc-4d73-f511-859bce8769f2"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\", max_tokens=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19acd7b4",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- In this notebook, we have used NVIDIA NIM microservices from the NVIDIA API Catalog.\n",
    "- The other APIs, ChatNVIDIA, NVIDIAEmbedding, and NVIDIARerank, also support self-hosted NIM microservices.\n",
    "- Change the `base_url` to your deployed NIM URL.\n",
    "- Example: `llm = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama3-8b-instruct\")`\n",
    "- NIM can be also hosted locally using Docker, following the [NVIDIA NIM for LLMs](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html) documentation. This is only true if you are the son of a rich oil tycoon, and have a few H100s sitting around in your basement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2dcf372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# connect to an LLM NIM running at localhost:8000, specifying a specific model\n",
    "llm = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama3-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cc87a6-2f83-4652-95f1-cf349db8bad6",
   "metadata": {
    "id": "35cc87a6-2f83-4652-95f1-cf349db8bad6"
   },
   "source": [
    "### 2) Intialize the embedding\n",
    "NVIDIAEmbeddings is a client to NVIDIA embeddings models that provides access to a NVIDIA NIM for embedding. It can connect to a hosted NIM or a local NIM using a base URL\n",
    "\n",
    "We selected **NV-Embed-QA** as the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d88f7838-b9f9-4fc5-8779-84df6cb26017",
   "metadata": {
    "id": "d88f7838-b9f9-4fc5-8779-84df6cb26017"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "embedder = NVIDIAEmbeddings(model=\"NV-Embed-QA\", truncate=\"END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9862f2e-5055-4fe4-818d-708091243d74",
   "metadata": {
    "id": "b9862f2e-5055-4fe4-818d-708091243d74"
   },
   "source": [
    "### 3) Obtain dataset\n",
    "I love taxes and work! Lets steal information about taxes and work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64106ff",
   "metadata": {
    "id": "b64106ff"
   },
   "source": [
    "#### In the **DataHandler** class defined below, we can;\n",
    "\n",
    "A) Walk through a webpage and find all sub-webpages and scrape the parent and children,\n",
    "\n",
    "B) Extract texts, tables and images from pdfs\n",
    "\n",
    "C) Split documents \n",
    "\n",
    "- Real world documents can be very long, this makes it hard to fit in the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
    "\n",
    "- To handle this we’ll split the Document into chunks for embedding and vector storage. More on text splitting [here](https://python.langchain.com/v0.2/docs/concepts/#text-splitters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c07d429",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "8c07d429",
    "outputId": "86e892d9-a219-4dd6-8a18-71daf5ee84e9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.parse  # To handle URL joining\n",
    "import fitz\n",
    "\n",
    "from tqdm import tqdm\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader, DataFrameLoader, CSVLoader, UnstructuredTSVLoader\n",
    "\n",
    "class DataHandler:\n",
    "    \"\"\"\n",
    "    Masterfully handles data scraping, preprocessing, and other data-related functionalities in this notebook.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path=\"./data/csv\"):\n",
    "        self.webloaders = [] # tracks all urls that have been converted to langchain WebBaseLoaders.\n",
    "        self.tabular = [] # tracks all tabular data that has been discovered by scraper. Delivers a list of CSVLoader. Cowardly refusing to save to same list\n",
    "        self.raws = []\n",
    "        self.csv_path = csv_path\n",
    "\n",
    "        self.text_splitter = CharacterTextSplitter(chunk_size=1024, separator=\" \", chunk_overlap=64)\n",
    "\n",
    "        self.visited_urls = set()   # Classwide tracker to prevent repeated visits when scraping web\n",
    "        self.tabular_data = []      # Tracks all tabular_data data that has been discovered by scraper. Delivers a list of CSVLoader. Cowardly refusing to save to same list.\n",
    "        self.textual_data = []      # Tracks all textual data scraped from websites or pdfs\n",
    "        self.all_data = []  # Just defining a joint list here instead of later during the functional call; in case we do not care about seperating them anymore (both are lists of Documents, just from different\n",
    "        # base sources)\n",
    "\n",
    "        if not os.path.exists(self.csv_path):\n",
    "            os.mkdir(self.csv_path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_csv(csv_path):\n",
    "        \"\"\"\n",
    "        Small func to read from csv and produce CSVLoaders.\n",
    "        \"\"\"\n",
    "        df = pd.from_csv( csv_path) # so we can yoink its columns\n",
    "        loader = CSVLoader(file_path=csv_path,\n",
    "                                    csv_args={  'delimiter': ',',\n",
    "                                                'quotechar': '\"',\n",
    "                                                'fieldnames': [str(col) for col in df.columns]}\n",
    "                                    )\n",
    "        return loader\n",
    "\n",
    "    def csvs_to_loader(self, directory):\n",
    "        \"\"\"\n",
    "        walks through directory, finds all csvs, and saves it into self.tabular \n",
    "        \"\"\"\n",
    "        for dir, subdir, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    fp = os.path.join(dir, file)\n",
    "                    loader = self.from_csv(fp)\n",
    "                    self.tabular.append(loader)\n",
    "\n",
    "\n",
    "    def extract_table_elements(self, url, table_elements):\n",
    "        \"\"\"\n",
    "        Helper func to extract tabular data\n",
    "\n",
    "        Args:\n",
    "            - url: url the table is under\n",
    "            - table_elements: list of table elements\n",
    "        \"\"\"\n",
    "        for table_idx, table in enumerate(table_elements): #TODO: Find better way to index different tables on the same page? not all have class attributes we can ID them with.\n",
    "            try:\n",
    "                tags = table.find_all('sup')\n",
    "                for tag in tags:\n",
    "                    tag.extract()\n",
    "\n",
    "                # Attempt to find the title of this table. tableheader elements only tell us (pandas) how to index it, but what we need the header element for context on what this table is about\n",
    "                tablename = f\"table_{table_idx}\" # default name presuming none is found\n",
    "                for headertype in ['h3', 'h4']: # unlikely to lie in h2 or h1? could result in duplicate data. if we find by those.\n",
    "                    header = table.find_previous(headertype)\n",
    "                    if header is not None and header.text is not None: # find the closest header\n",
    "                        tablename = self.clean_text(header.text).replace(\"#\", '').replace(\"/\", '')\n",
    "                        break\n",
    "                \n",
    "                tablename = os.path.basename(url) + f\" {tablename}\" # what we will call this table, some has really annoying spacing, so maybe .replace(' ', '')?\n",
    "                print(f\"Grabbing table data under tablename {tablename}\")\n",
    "                df = pd.read_html(StringIO(str(table)), header=0)[0] # some tables do not have tableheader <th> tags for first row \n",
    "                # which would result in  generic column indices being created, so forcibly set first row as tableheader. \n",
    "                df['context'] = [tablename] * df.shape[0]\n",
    "\n",
    "                if len(tablename) >= os.pathconf('/', 'PC_NAME_MAX'): # prevent shit from exploding because my tablename is damn scuffed but what todo.\n",
    "                    tablename = tablename[:os.pathconf('/', 'PC_NAME_MAX') - 10]\n",
    "                csv_path = os.path.join(self.csv_path, tablename + '.csv')\n",
    "\n",
    "                # print(\"csv_path:\", csv_path)\n",
    "                df.to_csv( csv_path, index=False ) # if true will fuck up columning in csvloader\n",
    "                # loader = UnstructuredTSVLoader(csv_path, mode='elements')\n",
    "                loader = CSVLoader(file_path=csv_path,\n",
    "                                    csv_args={  'delimiter': ',',\n",
    "                                                'quotechar': '\"',\n",
    "                                                'fieldnames': [str(col) for col in df.columns]}\n",
    "                                    )\n",
    "                for row in loader.load()[1:]: # first row is just column indexes, so void\n",
    "                    self.tabular.append(\n",
    "                        self.text_splitter.split_text(\n",
    "                            self.clean_text(row.page_content)\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "            except BaseException as e:\n",
    "                print(f\"Unable to extract table data from url {url} with error {e}, passing!\")\n",
    "\n",
    "    def create_loaders(self):\n",
    "        \"\"\"\n",
    "        Seperate method to create the loaders. Directly appends to textual_data attribute and calls extract_table_elements to handle tabular data.\n",
    "        \"\"\"\n",
    "        for url, soupy_little_guy in self.raws:\n",
    "            loader = WebBaseLoader(\n",
    "                    web_paths=(url,),  # No URL fetching as we already have the HTML content\n",
    "                    bs_kwargs={\"parse_only\": SoupStrainer(['main'])},\n",
    "                )\n",
    "            html_content = loader.load()\n",
    "            for i in range(len(html_content)):\n",
    "                self.textual_data.extend(\n",
    "                    self.text_splitter.split_text(\n",
    "                        self.clean_text(\n",
    "                            html_content[i].page_content\n",
    "                )))\n",
    "\n",
    "            table_elements = soupy_little_guy.find_all(\"table\") \n",
    "            self.extract_table_elements(url=url, table_elements=table_elements)\n",
    "\n",
    "\n",
    "    def scrape_website(self, base_url, max_depth, depth=0):\n",
    "        \"\"\"\n",
    "        Wraps around a nested function get_from_website, so when ran will define a new function that knows that the sauce base_url is.\n",
    "        Scuffed? Yes.\n",
    "        \"\"\"\n",
    "\n",
    "        def get_from_website(url, max_depth, depth):\n",
    "            \"\"\"\n",
    "            Recursively scrape a website by visiting links starting from url. \n",
    "            Because this is a mostly I/O bound operation, we make a seperate method that actually creates the Loaders.\n",
    "\n",
    "            Parameters:\n",
    "                - url:              URL to start scraping from\n",
    "                - depth:            Current recursion depth\n",
    "                - max_depth:        Maximum recursion depth to avoid infinite loops\n",
    "\n",
    "            Returns:\n",
    "                - appends to self.raws, [url, BeautifulSoup object] created from response content.\n",
    "            \"\"\"\n",
    "            if url in self.visited_urls or depth > max_depth:\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                soupy_little_guy = BeautifulSoup(response.content, 'html.parser')\n",
    "                if response.status_code != 200:\n",
    "                    return print(f\"Failed to retrieve {url}\")\n",
    "            except Exception as e:\n",
    "                return print(f\"Error accessing {url} with error: {e}\")\n",
    "                \n",
    "\n",
    "            self.visited_urls.add(url)\n",
    "            print(\"Current url:\", url)\n",
    "            self.raws.append([url, soupy_little_guy])\n",
    "\n",
    "            for link in soupy_little_guy.find_all('a', href=True):  # Find all links on the current page\n",
    "                relative_url = link['href']\n",
    "                absolute_url = urllib.parse.urljoin(url, relative_url)\n",
    "                if base_url in absolute_url:  # Avoids external sites\n",
    "                    get_from_website(absolute_url, max_depth, depth + 1)\n",
    "\n",
    "        get_from_website(base_url, max_depth, depth)\n",
    "\n",
    "\n",
    "    def scrape_pdf(self, pdf_path):\n",
    "        \"\"\"\n",
    "        Extracts information from pdf files.\n",
    "        Texts stay as texts.\n",
    "        Tables and images...\n",
    "\n",
    "        Parameters:\n",
    "            - pdf_path:     PDF file to extract from\n",
    "\n",
    "        Output:\n",
    "            - textual_data: List of cleaned strings that represent data from pdf doc\n",
    "        \"\"\"\n",
    "        try:\n",
    "            pdf_document = fitz.open(pdf_path)\n",
    "        except:\n",
    "            return print(f\"Unable to open {pdf_path}\")\n",
    "        print(f\"Current pdf: {pdf_path}\")\n",
    "        pdf_text = \"\"\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            pdf_text += page.get_text(\"text\")\n",
    "\n",
    "        self.textual_data.extend(\n",
    "            self.text_splitter.split_text(\n",
    "                self.clean_text(pdf_text)\n",
    "        ))\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Cleans text retrieved from sources to reduce the storage needed\n",
    "\n",
    "        get_from_website(base_url, max_depth, depth)\n",
    "        Parameters:\n",
    "            - text:         Original string\n",
    "\n",
    "        Output:\n",
    "            - cleaned_text: Cleaned string \n",
    "        \"\"\"\n",
    "        return text.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').replace('  ', ' ')\n",
    "    \n",
    "    def embed_text(self, text):\n",
    "        \"\"\"\n",
    "        Embeds a given text if necessary\n",
    "\n",
    "        Parameters:\n",
    "            - text:     Original string\n",
    "\n",
    "        Output:\n",
    "            - text:     Embedded string \n",
    "        \"\"\"\n",
    "        return embedder.embed_query(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec1d94c",
   "metadata": {},
   "source": [
    "We define our websites and pdf links below to call the dataloader on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e340c158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current url: https://www.iras.gov.sg/taxes\n",
      "Current url: https://www.iras.gov.sg/schemes\n",
      "Current url: https://www.mom.gov.sg/passes-and-permits\n",
      "Current url: https://www.mom.gov.sg/employment-practices\n",
      "Current url: https://www.mom.gov.sg/workplace-safety-and-health\n",
      "Current pdf: ./data/pdfs\\verification-checklist-inspection-of-machines.pdf\n"
     ]
    }
   ],
   "source": [
    "# Max depth determines whether you wish to look into children of parents websites, else set to 0\n",
    "max_depth = 0\n",
    "websites = [\n",
    "    \"https://www.iras.gov.sg/taxes\",\n",
    "    \"https://www.iras.gov.sg/schemes\",\n",
    "    \"https://www.mom.gov.sg/passes-and-permits\",\n",
    "    \"https://www.mom.gov.sg/employment-practices\",\n",
    "    \"https://www.mom.gov.sg/workplace-safety-and-health\"\n",
    "]\n",
    "\n",
    "datahandler = DataHandler()\n",
    "for website in websites:\n",
    "    datahandler.scrape_website(website, max_depth)\n",
    "\n",
    "if os.path.isdir('./data/pdfs'):\n",
    "    for pdf in os.listdir('./data/pdfs'):\n",
    "        datahandler.scrape_pdf(os.path.join('./data/pdfs', pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72414862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing table data under tablename passes-and-permits table_0\n",
      "Unable to extract table data from url https://www.mom.gov.sg/passes-and-permits with error module 'os' has no attribute 'pathconf', passing!\n",
      "Grabbing table data under tablename passes-and-permits Overseas Networks & Expertise Pass\n",
      "Unable to extract table data from url https://www.mom.gov.sg/passes-and-permits with error module 'os' has no attribute 'pathconf', passing!\n",
      "Grabbing table data under tablename passes-and-permits Work Permit for performing artiste\n",
      "Unable to extract table data from url https://www.mom.gov.sg/passes-and-permits with error module 'os' has no attribute 'pathconf', passing!\n",
      "Grabbing table data under tablename passes-and-permits Training Work Permit\n",
      "Unable to extract table data from url https://www.mom.gov.sg/passes-and-permits with error module 'os' has no attribute 'pathconf', passing!\n",
      "Grabbing table data under tablename passes-and-permits Letter of Consent for Dependant’s Pass holders who are business owners\n",
      "Unable to extract table data from url https://www.mom.gov.sg/passes-and-permits with error module 'os' has no attribute 'pathconf', passing!\n"
     ]
    }
   ],
   "source": [
    "datahandler.create_loaders() # only when we actually want to create loaders to debug stuff, so we don't need to scrape within the same function that we're (likely) debugging, saving time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9a8d2",
   "metadata": {},
   "source": [
    "And then, we embed the textual and tabular data gathered for use in the vector db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b763dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding textual data...: 100%|██████████| 13/13 [00:07<00:00,  1.66it/s]\n",
      "Embedding tabular data...: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "datahandler.embedded_data = [datahandler.embed_text(text) for text in tqdm(datahandler.textual_data, desc='Embedding textual data...')] + [datahandler.embed_text(text) for text in tqdm(datahandler.tabular_data, desc='Embedding tabular data...')]\n",
    "datahandler.textual_data.extend(datahandler.tabular_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b16b3-43ac-4269-9f37-05a33efe24fb",
   "metadata": {
    "id": "851b16b3-43ac-4269-9f37-05a33efe24fb"
   },
   "source": [
    "### 4) Storing the documents\n",
    "\n",
    "To build our foundational knowledge base from our collected data and allow for faster retrieval of vector queries, we need to have some form of search system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687a9681",
   "metadata": {},
   "source": [
    "#### a) Process the documents into vectorstore and save it to disk\n",
    "\n",
    "Vectorstores are good when we wish to store small datasets mainly in memory, thus using less storage.\n",
    "\n",
    "In this case, we use FAISS, which is a high-performace library that is efficient for similarity search and clustering of dense vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "804c85f6-181b-4291-a685-d6b378015544",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "804c85f6-181b-4291-a685-d6b378015544",
    "outputId": "ec8742d2-d131-4dfc-ab1e-854694194e0e"
   },
   "outputs": [],
   "source": [
    "# Here we create a faiss vector store from the documents and save it to disk.\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# You will only need to do this once, later on we will restore the already saved vectorstore\n",
    "store = FAISS.from_texts(datahandler.textual_data, embedder)\n",
    "VECTOR_STORE = './data/nv_embedding'\n",
    "store.save_local(VECTOR_STORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f867df18-11c8-45ea-b81c-1603459431f9",
   "metadata": {
    "id": "f867df18-11c8-45ea-b81c-1603459431f9"
   },
   "source": [
    "To enable runtime search, we index text chunks by embedding each document split and storing these embeddings in a vector database. Later to search, we embed the query and perform a similarity search to find the stored splits with embeddings most similar to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25619cae",
   "metadata": {},
   "source": [
    "Then, we can read the previously processed and saved vector store back for use;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5de3e07d-5fbe-4fe7-8f23-ed0b082f2413",
   "metadata": {
    "id": "5de3e07d-5fbe-4fe7-8f23-ed0b082f2413"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Load the FAISS vectorestore back.\n",
    "VECTOR_STORE = './data/nv_embedding'\n",
    "store = FAISS.load_local(VECTOR_STORE, embedder, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dbd352",
   "metadata": {},
   "source": [
    "#### b) Store all our data into a vectordb (connects vector stores to structured db systems)\n",
    "\n",
    "We will be using **Milvus DB** since it is an open-source, distributed vector DB designed for high-performance vector similarity search across massive datasets (which is what we kind of will have), and makes use of popular libraries like FAISS or Annoy for its vector searching.\n",
    "\n",
    "We have the option to use;\n",
    "1) MilvusVectorStore (Milvus Lite) which is easier to implement and can be more easily tied with ML frameworks --> Milvus-lite does not have a windows version (only supports linux and mac)\n",
    "2) MilvusClient (Python SDK) WITHOUT Docker which was used in a demo to build a RAG system --> Milvus-lite does not have a windows version (only supports linux and mac)\n",
    "3) Direct DB WITH Docker (Milvus Standalone) to directly manage and control the connections as well as increase flexibility --> What im using first since it is the most straighforward (im trying to find if milvus standalone can be used locally instead of through docker)\n",
    "\n",
    "Sources used:\n",
    "- https://milvus.io/docs/quickstart.md\n",
    "- https://milvus.io/docs/integrate_with_langchain.md\n",
    "- https://milvus.io/docs/multimodal_rag_with_milvus.md\n",
    "- https://github.com/milvus-io/bootcamp/tree/master/bootcamp/tutorials/quickstart/apps/multimodal_rag_with_milvus (Possible demo to use)\n",
    "\n",
    "##### Note:\n",
    "Previously, I thought of using **Weaviate** because it is a free, open-source, scalable and reliable vector database service with decent amounts of documentation online.\n",
    "\n",
    "The Weaviate DB works more with Object-Oriented design rather than rows/columns and does not need to be structured like an SQL DB since it already has integrated models/pre-computed embeddings to handle vector embeddings.\n",
    "\n",
    "However, I realised it **could not** be GPU accelerated during its vector retrieval/indexing.\n",
    "\n",
    "Sources used:\n",
    "- https://python.langchain.com/docs/integrations/vectorstores/weaviate/\n",
    "- https://weaviate.io/developers/weaviate/client-libraries/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52344def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-milvus\n",
    "# !pip install -U pymilvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e5c2dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient, DataType, CollectionSchema, FieldSchema\n",
    "import hashlib\n",
    "\n",
    "class MilvusDB:\n",
    "    \"\"\"\n",
    "    Not-so-masterfully handles the vector database using Milvus and all DB related functionality.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, uri):\n",
    "        self.uri = uri\n",
    "        self.client = None\n",
    "        self.collection_name = None\n",
    "\n",
    "        self.similarity_threshold = 0.99  # Similarity between new input data and existing data\n",
    "        self.relevancy_threshold = 0.99   # Similarity between queried data and query\n",
    "        self.set_up_db()\n",
    "\n",
    "    def set_up_db(self):\n",
    "        \"\"\"\n",
    "        Starts up connection to Weaviate DB and creates schema if not already created\n",
    "\n",
    "        Parameters: None\n",
    "\n",
    "        Output:     None\n",
    "        \"\"\"\n",
    "        self.client = MilvusClient(uri=self.uri)\n",
    "\n",
    "    def load_collection(self):\n",
    "        \"\"\"\n",
    "        Loads collection into RAM for faster retrieval\n",
    "\n",
    "        Parameters: None\n",
    "\n",
    "        Output:     None\n",
    "        \"\"\"\n",
    "        self.client.load_collection(\n",
    "            collection_name=self.collection_name,\n",
    "            replica_number=1 # Number of replicas to create on query nodes. Max value is 1 for Milvus Standalone, and no greater than `queryNode.replicas` for Milvus Cluster.\n",
    "        )\n",
    "\n",
    "    def release_collection(self):\n",
    "        \"\"\"\n",
    "        Releases the collection from memory to save memory usage\n",
    "\n",
    "        Parameters: None\n",
    "\n",
    "        Output:     None\n",
    "        \"\"\"\n",
    "        self.client.release_collection(\n",
    "            collection_name=self.collection_name\n",
    "        )\n",
    "\n",
    "    def create_collection(self, collection_name, dimensions):\n",
    "        \"\"\"\n",
    "        Creates a new collection in the DB\n",
    "\n",
    "        Parameters: \n",
    "            - collection_name:  Name of collection to make\n",
    "            - dimensions:       Number of dimensions for vector data\n",
    "\n",
    "        Output:     None\n",
    "        \"\"\"     \n",
    "        # Checks if the client already has the collection in their Milvus instance\n",
    "        # if self.client.has_collection(collection_name):  # TODO: Change this later\n",
    "        #     self.client.drop_collection(collection_name)\n",
    "\n",
    "        # Defines a schema to follow when creating the DB\n",
    "        id_field = FieldSchema(name=\"id\", dtype=DataType.VARCHAR, max_length=512, is_primary=True)\n",
    "        embedding_field = FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=dimensions)\n",
    "        text_field = FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=1152)  # Use VARCHAR for string types\n",
    "        source_field = FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=1152)  # Optional metadata field\n",
    "        schema = CollectionSchema(fields=[id_field, embedding_field, text_field, source_field])\n",
    "\n",
    "        # Creates the collection\n",
    "        self.client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            dimension=dimensions,\n",
    "            schema=schema,\n",
    "            metric_type=\"COSINE\",               # Inner product distance; or COSINE or L2 (Euclidean)\n",
    "            consistency_level=\"Strong\",     # Strong consistency level\n",
    "        )\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "        # Creates an index for more efficient similarity search later on based on the metric_type and index_type\n",
    "        self.index_params = MilvusClient.prepare_index_params()\n",
    "        self.index_params.add_index(\n",
    "            field_name=\"embedding\",\n",
    "            metric_type=\"COSINE\",\n",
    "            index_type=\"IVF_FLAT\",\n",
    "            index_name=\"embedding_index\",\n",
    "            params={ \"nlist\": 128 }\n",
    "        )\n",
    "        self.client.create_index(\n",
    "            collection_name=self.collection_name,\n",
    "            index_params=self.index_params,\n",
    "            sync=False\n",
    "        )\n",
    "\n",
    "    def insert_data(self, original, embedded):\n",
    "        \"\"\"\n",
    "        Adds document and embedding object pairs to the DB collection if not alreadt inside \n",
    "\n",
    "        Parameters:\n",
    "            - original:     Original documents\n",
    "            - embedded:     Embedded documents\n",
    "\n",
    "        Output: None\n",
    "        \"\"\"\n",
    "        self.load_collection()\n",
    "        data = []\n",
    "\n",
    "        for i, embedded_line in enumerate(tqdm(embedded, desc=\"Inserting data...\")):\n",
    "            unique_id = self.generate_unique_id(embedded_line)\n",
    "            if not self.check_for_similar_vectors(embedded_line) and not self.check_for_similar_ids(unique_id):\n",
    "                data.append({\"id\": unique_id, \"embedding\": embedded_line, \"text\": original[i], \"source\": \"None\"})\n",
    "\n",
    "        self.client.insert(collection_name=self.collection_name, data=data)\n",
    "        self.release_collection()\n",
    "\n",
    "    def generate_unique_id(self, data):\n",
    "        \"\"\"\n",
    "        Generates a unique hash ID based on the vector or text data\n",
    "\n",
    "        Parameters:\n",
    "            - data: The vector or text data used to generate the hash\n",
    "\n",
    "        Returns:\n",
    "            - id:   A unique hash ID as a string\n",
    "        \"\"\"\n",
    "        data_str = str(data)\n",
    "        unique_id = hashlib.sha256(data_str.encode()).hexdigest()\n",
    "        return unique_id\n",
    "\n",
    "    def check_for_similar_vectors(self, embedding, top_k=5):\n",
    "        \"\"\"\n",
    "        Checks the DB for vectors that are similar to the input embedding (based on distance metric like cosine similarity or Euclidean distance)\n",
    "\n",
    "        Parameters:\n",
    "            - embedding:    Embedded documents\n",
    "            - top_k:        Top K number of documents that are similar to the input embedding\n",
    "\n",
    "        Output: \n",
    "            - check:        True or False value of whether a similar vector has been found\n",
    "        \"\"\"\n",
    "        try:\n",
    "            search_params = {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}}\n",
    "            results = self.client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                data=[embedding],\n",
    "                anns_field=\"embedding\",  # Adjust field name based on your Milvus schema\n",
    "                search_params=search_params,\n",
    "                limit=top_k\n",
    "            )\n",
    "            for result in results:\n",
    "                for vector in result:\n",
    "                    if vector['distance'] >= self.similarity_threshold:\n",
    "                        return True  \n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking if vector exists: {e}\")\n",
    "            return False\n",
    "\n",
    "    def check_for_similar_ids(self, id):\n",
    "        \"\"\"\n",
    "        Checks the DB for ids that are similar to the new one created\n",
    "\n",
    "        Parameters:\n",
    "            - id:       Unique id generated for new row\n",
    "\n",
    "        Output: \n",
    "            - check:    True or False value of whether a similar vector has been found\n",
    "        \"\"\"\n",
    "        try:\n",
    "            results = self.client.query(\n",
    "                collection_name=self.collection_name,\n",
    "                filter=f\"id == '{id}'\",  \n",
    "                output_fields=[\"id\"], limit=1000 \n",
    "            )\n",
    "            return True if results else False\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking if ID exists: {e}\")\n",
    "            return False\n",
    "\n",
    "    def retrieve_data(self, question):\n",
    "        \"\"\"\n",
    "        Retrieves vector data from DB based on embedded question\n",
    "\n",
    "        Parameters:\n",
    "            - question:     Embedded question as a vector\n",
    "\n",
    "        Output:\n",
    "            - search_res:   Results of vector retrieval\n",
    "        \"\"\"\n",
    "        self.load_collection()\n",
    "        search_res = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            data=[question],  \n",
    "            limit=3,  # Return top 3 results\n",
    "            search_params={\"metric_type\": \"COSINE\", \"params\": {}},  # Inner product distance\n",
    "            output_fields=[\"text\"],  # Return the text field\n",
    "        )\n",
    "        filtered_res = [res for res in search_res[0] if res['distance'] > self.relevancy_threshold]\n",
    "        self.release_collection()\n",
    "        return filtered_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "89671eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting data...: 100%|██████████| 13/13 [00:05<00:00,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "database = MilvusDB('http://localhost:19530')\n",
    "database.create_collection(\"Documents\", 1024)\n",
    "database.insert_data(datahandler.textual_data + datahandler.tabular_data, datahandler.embedded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a41ff63-6adc-4055-8bc4-e7ecaad0fb4d",
   "metadata": {
    "id": "4a41ff63-6adc-4055-8bc4-e7ecaad0fb4d"
   },
   "source": [
    "### 5) Using data to answer questions\n",
    "\n",
    "With our stored embedded data, we can retrieve relevant vectors stored in our vectorstore/db to answer embedded questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9ce3ba",
   "metadata": {},
   "source": [
    "#### A) Using vectorstore to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa362c9-48ab-4646-bc29-bc2aca92505d",
   "metadata": {
    "id": "5aa362c9-48ab-4646-bc29-bc2aca92505d"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "context = store.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer solely based on the following:\\n<Documents>\\n{context}\\n</Documents>\",\n",
    "        ),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Langchain's LCEL(LangChain Expression Language) Runnable protocol is used to define the chain\n",
    "# LCEL allows pipe together components and functions\n",
    "chain = (\n",
    "    {\"context\": context, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a0191d",
   "metadata": {},
   "source": [
    "#### B) Using VectorDB to answer\n",
    "\n",
    "What happens if our vectordb does not have the relevant information needed though?\n",
    "\n",
    "#### C) Using web sources to answer\n",
    "\n",
    "Using Tavily, we can create a search engine that calls abstract searching, scraping, filtering and extracting from online sources.\n",
    "\n",
    "Only issue is, it takes too long to get the answer out, so it should only be used as a final resort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3048726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7acd9c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, MessagesState\n",
    "from typing import Literal\n",
    "\n",
    "class SearchEngine:\n",
    "    def __init__(self):\n",
    "        self.search_tool = TavilySearchResults(max_results=3)\n",
    "        self.llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "\n",
    "        self.tools = [self.search_tool]  # Add other tools here if needed\n",
    "        self.llm_with_tools = self.llm.bind_tools(self.tools)\n",
    "        self.workflow = self.build_workflow()\n",
    "\n",
    "    def build_workflow(self):\n",
    "        \"\"\"Builds the state graph workflow.\"\"\"\n",
    "        workflow = StateGraph(MessagesState)\n",
    "        \n",
    "        # Add nodes for agent (LLM) and tools (search)\n",
    "        workflow.add_node(\"agent\", self.call_model)\n",
    "        workflow.add_node(\"tools\", ToolNode(self.tools))\n",
    "        \n",
    "        # Add conditional logic to determine if tools should be used\n",
    "        workflow.add_edge(\"__start__\", \"agent\")\n",
    "        workflow.add_conditional_edges(\"agent\", self.should_continue)\n",
    "        workflow.add_edge(\"tools\", \"agent\")\n",
    "        \n",
    "        return workflow.compile()\n",
    "\n",
    "    def call_model(self, state):\n",
    "        \"\"\"Invoke the LLM with the current state.\"\"\"\n",
    "        messages = state[\"messages\"]\n",
    "        response = self.llm_with_tools.invoke(messages)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    def should_continue(self, state):\n",
    "        \"\"\"Determine whether to continue invoking tools or end the workflow.\"\"\"\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        if last_message.tool_calls:\n",
    "            return \"tools\"\n",
    "        return \"__end__\"\n",
    "\n",
    "    def query(self, user_query, location_context=' in singapore'):\n",
    "        \"\"\"Main method to process the user's query.\"\"\"\n",
    "        initial_state = MessagesState({\"messages\": [self.llm_with_tools.invoke(user_query + location_context)]})\n",
    "        return self.workflow.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "198905d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question):\n",
    "    retrieved_data = database.retrieve_data(embedder.embed_query(question))\n",
    "    if len(retrieved_data) > 0:\n",
    "        retrieved_lines_with_distances = [\n",
    "            (res[\"entity\"][\"text\"], res[\"distance\"]) for res in retrieved_data[0]\n",
    "        ]\n",
    "        context = '\\n'.join(\n",
    "            [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances]\n",
    "        )\n",
    "        prompt = f\" \\\n",
    "            Context from database:\\n{context}\\n\\n\\\n",
    "            User's question: {question}\\n\\n\\\n",
    "            Please generate an informative response.\\\n",
    "        \"\n",
    "        return llm.invoke(prompt).content\n",
    "    print(\"No data found in database. Searching online...\")\n",
    "    search_engine = SearchEngine()\n",
    "    response = search_engine.query(question)\n",
    "\n",
    "    for message in response['messages']:\n",
    "        print(message.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08866cc3",
   "metadata": {},
   "source": [
    "#### Now, we can try asking different types of questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d318b0",
   "metadata": {
    "id": "18d318b0"
   },
   "source": [
    "Case 1: Irrelevant question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9798ab63",
   "metadata": {
    "id": "9798ab63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no information provided in the documents about a component or entity called \"two hundred and twenty five million.\" The documents contain information about various financial transactions, mortgage and loan balances, estate duty calculations, and rules related to M&A allowance and motor vehicle expenses, but there is no mention of a \"component\" with a value of 225 million.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"What is component two hundred and twenty five million?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f474d0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't find that information.  Is there anything else that I can help you with?\n",
      "\n",
      "[{\"url\": \"https://www.nbcnews.com/\", \"content\": \"judge asks in granting bail\\nAsian America\\nBrooklyn woman arrested after allegedly throwing hot coffee at a man in a Palestinian scarf\\nU.S. news\\nMichigan police make arrest in the disappearance of a woman missing since 2021\\nAsian America\\nBrooklyn woman arrested after allegedly throwing hot coffee at a man in a Palestinian scarf\\nU.S. news\\nMichigan police make arrest in the disappearance of a woman missing since 2021\\nHealth\\nHealth news\\nMysterious dog respiratory illness may be caused by a new type of bacterial infection, researchers say\\nCoronavirus\\nFour more free Covid tests will be available to U.S. households\\nHealth news\\nThis type of belly fat is linked to increased risk of Alzheimer's, research finds\\nHealth news\\nNew weight loss drugs change how people think of Thanksgiving and other holiday meals\\nHealth news\\nDeadly listeria outbreak linked to whole peaches, nectarines and plums\\nHealth news\\nNew weight loss drugs change how people think of Thanksgiving and other holiday meals\\nHealth news\\nDeadly listeria outbreak linked to whole peaches, nectarines and plums\\nPop culture\\nCulture & Trends\\nComedian Matt Rife responds to Netflix special backlash with link to ‘special needs helmets’\\nU.S. news\\nA stampede, shooting and bodyguard brawls: How Sean Combs' legal troubles have played out\\nOUT News\\nAntoni Porowski and fiancé Kevin Harrington split 1 year after getting engaged\\nAsian America\\nGroom secretly learned Korean for over a year to surprise wife and family at wedding\\nCulture & Trends\\n'We just had to do it': Bradley Cooper speaks out about prosthetic nose controversy in 'Maestro'\\nAsian America\\nGroom secretly learned Korean for over a year to surprise wife and family at wedding\\nCulture & Trends\\n'We just had to do it': Bradley Cooper speaks out about prosthetic nose controversy in 'Maestro'\\nBusiness, Tech & Media\\nEXCLUSIVE\\nDiageo cites Diddy rape claims in renewed push to keep him out of tequila ads\\n#MeToo Reckoning\\nDiddy’s companies named in suit accusing an ex-Bad Boy president of sexual assault\\nTech News\\nWith X under fire, Elon Musk digs in and finds support from conservatives\\nTech News\\nSam Altman returns as OpenAI CEO after days of infighting at AI startup\\nU.S. news\\nHow Formula One accidentally helped Las Vegas workers land the 'best contract ever'\\nTech News\\nSam Altman returns as OpenAI CEO after days of infighting at AI startup\\nU.S. news\\nHow Formula One accidentally helped Las Vegas workers land the 'best contract ever'\\nNBC OUT\\nOUT News\\nAntoni Porowski and fiancé Kevin Harrington split 1 year after getting engaged\\nOUT Pop Culture\\nIn 'The Dads,' men bond over fishing and their love for their transgender kids\\nOUT Politics and Policy\\nTeachers and students grapple with fears and confusion about new laws restricting pronoun use\\nGay bar druggings\\nVictims of the deadly druggings that terrorized NYC's gay bars are haunted by unknowns\\nAsian America\\nCulture & Trends\\nNetflix’s ‘Squid Game: The Challenge’ turns dystopian drama into real-life competition — minus the death\\nAsian America\\nThanksgiving fusion dishes have become a way for Asian Americans to share their cultures with others\\nImmigration\\nMore asylum claims, illegal crossings along U.S.-Canada border, despite the dangers\\nAsian America\\nBrooklyn woman arrested after allegedly throwing hot coffee at a man in a Palestinian scarf\\nNBCBLK\\nNBCBLK\\n Profile\\nSections\\ntv\\nFeatured\\nMore From NBC\\nFollow NBC News\\nnews Alerts\\nThere are no new alerts at this time\\nHostage release and Gaza truce to start Friday, Qatar says\\nLIVEAl-Shifa Hospital director arrested by Israeli forces\\nLIVEAl-Shifa Hospital director arrested by Israeli forces\\nHostage deal exposes tensions between Israel’s far-right and hostage families\\nGazans welcome deal to pause fighting but dread what may come afterward\\nFringe proposal to displace Palestinians draws condemnation and triggers past trauma\\nOne of the only places that can claim to host the First Thanksgiving rejects the label\\nOne of the only places that can claim to host the First Thanksgiving rejects the label\\nNew York City Mayor Eric Adams accused of sexually assaulting a colleague in 1993\\nNew York City Mayor Eric Adams accused of sexually assaulting a colleague in 1993\\nSean 'Diddy' Combs accused of drugging, sexually assaulting woman when she was in college\\nSean 'Diddy' Combs accused of drugging, sexually assaulting woman when she was in college\\nNetflix’s ‘Squid Game: The Challenge’ turns dystopian drama into real-life competition — minus the death\\nChinese hospitals ‘overwhelmed with sick children’ after surge in respiratory illness\\nChinese hospitals ‘overwhelmed with sick children’ after surge in respiratory illness\\nFor teen girls victimized by ‘deepfake’ nude photos, there are few pathways to recourse\\nFor teen girls victimized by ‘deepfake’ nude photos, there are few pathways to recourse\\nTrump rages about judge and law clerk while gag order is paused in New York fraud trial\\nTrump rages about judge and law clerk while gag order is paused in New York fraud trial\\nLong-awaited hostage deal set to move ahead\\nLong-awaited hostage deal set to move ahead\\nBlack Friday\\nThe best Black Friday deals on Amazon\\nBlack Friday\\nThe best early Black Friday deals to shop right now\\nLuggage\\n8 highly rated work totes for commuting with ease\\nBlack Friday\\nThe best Black Friday deals at Target to shop right now\\nBlack Friday\\nThe best Walmart Black Friday deals to shop right now\\n The best Walmart Black Friday deals to shop right now\\nCaptives of Cannabis Part II: Mass Murder, Migrants & Money Laundering\\nLatest News\\nHead of security for sons of 'El Chapo' arrested in Mexico, sought for extradition\\nAlabama priest who fled to Italy with teen he later married is expected to be defrocked\\nSean 'Diddy' Combs accused of drugging, sexually assaulting woman when she was in college\\nHostage deal exposes tensions between Israel’s far-right and hostage families\\nNew York City Mayor Eric Adams accused of sexually assaulting a colleague in 1993\\n'Stop the rancor': Biden calls for national unity on Thanksgiving Day\\nTrump rages about judge and law clerk while gag order is paused in New York fraud trial\\nBatting cage accident at Georgia high school leaves a player in a coma\\nChinese hospitals ‘overwhelmed with sick children’ after surge in respiratory illness\\nNetflix’s ‘Squid Game: The Challenge’ turns dystopian drama into real-life competition — minus the death\\nFor teen girls victimized by ‘deepfake’ nude photos, there are few pathways to recourse\\nAs holiday shopping begins, Black business owners hope shoppers don’t forget them\\nGerman police raid homes of far-right Reich Citizens group\\nMacy’s Thanksgiving Day Parade recap: All the floats, performances and more\\nOne of the only places that can claim to host the First Thanksgiving rejects the label\\nThis Afro Latina actor touts the power behind journaling\\nThanksgiving travelers told to 'be nice' as all-time passenger record set to be broken\\nSmall Kentucky town urged to evacuate after train derails, spilling chemicals\\nIn a shock for Europe, anti-Islam populist Geert Wilders records a massive win in Dutch elections\\nGaza hostage release and truce to start Friday, Qatar says\\nUC Berkeley men's basketball player confronts fan who allegedly called him a 'terrorist'\\nAustralia has too many sheep — and farmers are giving them away for free\\nCharleston, South Carolina, elects its first Republican mayor since Reconstruction Era\\nNorth Korea says it is suspending military pact with South\\nBiden and Harris to attend services for Rosalynn Carter in Georgia\\nWhat causes skin to itch? Remembering Rosalynn Carter, former first lady and tireless humanitarian\\nTop videos\\nNightly News\\nLong-awaited hostage deal set to move ahead\\nNightly News\\nIsraeli military arrests Al-Shifa Hospital director, 200 patients too sick to evacuate\\nNightly News\\nAuthorities looking into airline incidents amid Thanksgiving travel rush\\nNightly News\\nFormer Obama advisor caught on video berating NYC street vendor\\nMost popular\\nTrump investigations\\n'Deluge' of threats against judge and law clerk in Trump’s civil fraud trial are detailed in new court filing\\nCelebrity\\nCuba Gooding Jr. faces 2 sexual assault lawsuits in New York\\nU.S. news\\nHusband charged with murder more than two years after his wife vanished in Michigan\\nLive Updates\\nMacy’s As holiday shopping begins, Black business owners hope shoppers don’t forget them\\n#MeToo Reckoning\\nDiddy’s companies named in suit accusing an ex-Bad Boy president of sexual assault\\nNBCBLK\\nAlabama A&M will lead bands in Macy’s Thanksgiving Day Parade\\nNBCBLK\\nNFL disability program leaves retired Saints tight end hurting and angry\\nLatino\\nLatino\\nBrazilian officials investigating company behind Taylor Swift shows in Rio de Janeiro after fan's death\\nCulture & Trends\\nThis Afro Latina actor touts the power behind journaling\\nCulture & Trends\\nPuerto Ricans are pushing to make these unique slang words officially Spanish\\nImmigration\\nMore asylum claims, illegal crossings along U.S.-Canada border, despite the dangers\\nYour personalized news, including saved and local stories\\nAccess these features with your free account.\\n\"}, {\"url\": \"https://apnews.com/us-news\", \"content\": \"U.S. News\\nBlinken sees goals largely unfulfilled in Mideast trip, even as Israel pledges to protect civilians\\nKlete Keller, Olympic gold medalist swimmer, gets 6 months in home detention for Jan. 6 Capitol riot\\nBreaches by Iran-affiliated hackers spanned multiple U.S. states, federal agencies say\\nReport: Belief death penalty is applied unfairly shows capital punishment’s growing isolation in US\\nRetired Justice Sandra Day O’Connor, the first woman on the Supreme Court, has died at 93\\nTopics\\nAbortion\\nCourt pauses federal policy allowing abortion clinic operators to get grants -- but only in Ohio\\nMan pleads guilty to 2022 firebombing of Wisconsin anti-abortion office\\nWest Virginia places anti-abortion pregnancy center coalition at the helm of $1M grant program\\nOpponents gave input on ballot language for abortion-rights measure, Ohio elections chief says\\nLGBTQ+\\nPolice raid Moscow gay bars after a Supreme Court ruling labeled LGBTQ+ movement ‘extremist’\\nIowa court affirms hate crime conviction of man who left anti-gay notes at homes with rainbow flags\\nFirst same-sex married couple in Nepal vow to continue campaign for gay rights\\nFor a male sexual assault survivor, justice won in court does not equal healing\\nShootings\\nLas Vegas police search for suspect after 5 homeless people are shot, killing 1\\nLos Angeles police searching for suspect in three fatal shootings of homeless people\\nA world away from the West Bank, Vermont shooting victims and their families face new grief and fear\\nState trooper who fatally shot man at hospital likely prevented more injuries, attorney general says\\nImmigration\\nUS border officials are closing a remote Arizona crossing because of overwhelming migrant arrivals\\nTexas judge rips into Biden administration’s handling of border in dispute over razor wire barrier\\nCongress is eying immigration limits as GOP demands border changes in swap for Biden overseas aid\\nRepublicans want to pair border security with aid for Ukraine. Here’s why that makes a deal so tough\\nMore news\\nMassachusetts Republicans stall funding, again, to shelter the homeless and migrants\\nDefense head calls out those who advocate isolationism and ‘an American retreat from responsibility’\\nFormer prep school teacher going back to prison for incident as camp counselor\\nRacist remarks cited before cannabis regulator’s suspension, court documents say\\nAlabama woman pleads guilty in 2019 baseball bat beating death of man found in a barrel\\nWinter weather in Pacific Northwest cuts power to thousands in Seattle, dumps snow on Cascades\\nGroup of swing state Muslims vows to ditch Biden in 2024 over his war stance\\nFederal judge tosses lawsuit alleging environmental racism in St. James Parish\\nSheriff says Alabama family’s pet ‘wolf-hybrid’ killed their 3-month-old boy\\nVermont day care provider convicted of causing infant’s death with doses of antihistamine\\nRemington gun factory in operation for nearly 200 years is set to close\\nWest Virginia prison inmate indicted on murder charge in missing daughter’s death\\nBMW recalls SUVs after Takata air bag inflator blows apart, hurling shrapnel and injuring driver\\nSend-offs show Carlton Pearson’s split legacy spurred by his inclusive beliefs, rejection of hell\\nTeen girls are being victimized by deepfake nudes. Four are serving today\\nUS Navy plans to raise jet plane off Hawaii coral reef using inflatable cylinders\\nJudge rejects calls to halt winter construction work on Willow oil project in Alaska during appeal\\nUS proposes plan to help the snow-dependent Canada lynx before warming shrinks its habitat\\nFormer Colombian military officer accused in base bombing extradited to Florida\\nFormer Memphis officer charged in Tyre Nichols’ death had some violations in prior prison guard job\\nNew York could see more legal pot shops after state settles cases that halted market\\nEx-correctional officer at federal prison in California gets 5 years for sexually abusing inmates\\nMan who avoided prosecution as teen in 13-year-old’s killing found guilty of killing father of 2\\nJury orders egg suppliers to pay $17.7 million in damages for price gouging in 2000s\\nMovie armorer in ‘Rust’ fatal shooting pleads not guilty to unrelated gun charge\\nWisconsin Senate Democrats choose Hesselbein as new minority leader\\nAging dams in central and western Massachusetts to be removed in $25M project\\nCopyright 2023 The Associated Press. One family is pushing for more protections\\nCalifornia sheriff’s sergeant recovering after exchanging gunfire with suspect who was killed\\nTrump and DeSantis are at dueling campaign events in Iowa with the caucuses just six weeks away\\nJustice Sandra Day O’Connor paved a path for women on the Supreme Court. All Rights Reserved.\\n\"}, {\"url\": \"https://www.cbsnews.com/\", \"content\": \"From national issues to rising prices, a survey found Americans are feeling an additional strain this year\\nWorld\\nThe Israeli army, which raided the hospital last week, has alleged that Hamas fighters used a tunnel complex beneath the facility in Gaza City to stage attacks.\\n CBS Reports examines the crossroads of families fighting for a future, a city struggling to keep up, and a nation in the throes of a migration reckoning.\\n The Fulton County district attorney's office released the video of the August incident, where the now-former officer tasered Johnny Hollman multiple times at the scene of a car accident. The Fulton County district attorney's office released the video of the August incident, where the now-former officer tasered Johnny Hollman multiple times at the scene of a car accident. \\\"\\nGeert Wilders won on a platform that included a call for an EU membership referendum, a halt to accepting asylum-seekers and the \\\"de-Islamization\\\" of the Netherlands.\\n\"}]\n",
      "Based on the search results, it appears that there is no concrete information about the current events that triggered the long-awaited hostage deal. However, there is a mention of a situation where a hostage deal exposes tensions between Israel's far-right and hostage families.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ask(\"What is component two hundred and twenty five million?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3223cefb",
   "metadata": {
    "id": "3223cefb"
   },
   "source": [
    "Case 2: Simple questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ba4ef8",
   "metadata": {
    "id": "28ba4ef8"
   },
   "outputs": [],
   "source": [
    "print(chain.invoke(\"How do i file taxes for my company?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "38b9a008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found in database. Searching online...\n",
      "I don't have specific information about filing taxes for your company. However, I can guide you to a suitable resource in Singapore. You may wish to refer to the Inland Revenue Authority of Singapore (IRAS) for a step-by-step guide on how to file taxes in Singapore. \n",
      "\n",
      "Meantime, I will not make a function call in this instance as you can easily access the IRAS website yourself to obtain the necessary information.\n",
      "\n",
      "[{\"url\": \"https://www.pilotoasia.com/guide/personal-income-tax-singapore\", \"content\": \"Filing of tax returns is required if your annual income is or more. Starting from YA 2024, the top marginal Personal Income Tax rate will be increased from 22% to 24%. Starting from YA 2024, the personal income tax rate in Singapore for non-tax residents will be set at 24%. Apart from a few exceptions, overseas income is exempted from taxation.\"}, {\"url\": \"https://mytax.iras.gov.sg/\", \"content\": \"At AXS Stations if you are a DBS/POSB customer (for Individual Income Tax and Property Tax only). Submit forms to IRAS by completing: GIRO application forms for the different taxes. You may choose Master Giro application form if you wish to pay more than one of your taxes (Individual Income Tax, Property Tax, GST or Witholding Tax).\"}, {\"url\": \"https://dollarsandsense.sg/complete-guide-filing-personal-income-tax/\", \"content\": \"There are three main components to personal income tax filing that you need to know about. The 1) income you earn; 2) deductions that you qualify for; and 3) reliefs you are eligible for. #1 Income that you earn. All income earned in or derived from Singapore is chargeable to income tax.\"}]\n",
      "According to the IRAS website, filing of tax returns is required if your annual income is or more.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ask(\"How do i file taxes for my company?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0f962",
   "metadata": {
    "id": "87b0f962"
   },
   "source": [
    "Case 3: Complex questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c2e9c",
   "metadata": {
    "id": "af3c2e9c"
   },
   "outputs": [],
   "source": [
    "print(chain.invoke(\"In the event my foriegn employee is injured at work, how do i report the incident and claim reparations?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yxJMeKE-LItu",
   "metadata": {
    "id": "yxJMeKE-LItu"
   },
   "source": [
    "Case 4: Realistic questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "SM9lTRNlLItw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SM9lTRNlLItw",
    "outputId": "16202adf-df26-453a-f34e-820968f35832"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From 2025 onwards, as an employer whose firm qualifies for PWCS (Productivity Works Credits Scheme), you can receive co-funding from the government of up to 50% for the first tier of wage increases and 15% to 30% for the second tier of wage increases. This co-funding support applies to wage increases given in qualifying year 2025 and onwards. The gross monthly wage ceiling for PWCS co-funding will be increased to $3,000 in qualifying years 2025 and 2026.\\n\\nPlease note that the specific rates and details of the co-funding support may be subject to changes or updates in the PWCS guidelines. It is recommended to consult the official guidelines or contact the relevant authorities for the most accurate and up-to-date information.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"I am an employer whose firm qualifies for PWCs. From 2025 onwards, how much Co-Funding can I recieve from the government?\")\n",
    "# In its current state, the model does an ethan for this quite hard question, although it at least does retrieve some (few?) relevant PWCS info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3854c7-68a3-45b4-9e69-2c4e583d651f",
   "metadata": {
    "id": "9d3854c7-68a3-45b4-9e69-2c4e583d651f"
   },
   "source": [
    "### 6) Enhancing accuracy for single data sources\n",
    "\n",
    "This example demonstrates how a re-ranking model can be used to combine retrieval results and improve accuracy during retrieval of documents.\n",
    "\n",
    "Typically, reranking is a critical piece of high-accuracy, efficient retrieval pipelines. Generally, there are two important use cases:\n",
    "\n",
    "- Combining results from multiple data sources\n",
    "- Enhancing accuracy for single data sources\n",
    "\n",
    "Here, we focus on demonstrating only the second use case. If you want to know more, check [here](https://github.com/langchain-ai/langchain-nvidia/blob/main/libs/ai-endpoints/docs/retrievers/nvidia_rerank.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e8677e-a37f-42e2-8fea-4c4413f7d682",
   "metadata": {
    "collapsed": true,
    "id": "b7e8677e-a37f-42e2-8fea-4c4413f7d682"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIARerank\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# We will narrow the collection to 100 results and further narrow it to 10 with the reranker.\n",
    "retriever = store.as_retriever(search_kwargs={'k':100}) # typically k will be 1000 for real world use-cases\n",
    "ranker = NVIDIARerank(model='nv-rerank-qa-mistral-4b:1', top_n=10)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer solely based on the following context:\\n<Documents>\\n{context}\\n</Documents>\",\n",
    "        ),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "reranker = lambda input: ranker.compress_documents(query=input['question'], documents=input['context'])\n",
    "\n",
    "chain_with_ranker = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | {\"context\": reranker, \"question\": lambda input: input['question']}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1940aae",
   "metadata": {
    "id": "f1940aae",
    "outputId": "33a3f36f-5d7a-41a4-b919-56e724ff9f7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided documents, to file taxes for your company, you need to follow these steps:\n",
      "\n",
      "1. Ensure that you are duly authorized by your company as an 'Approver' for Corporate Tax (Filing and Applications) in Corppass. You can refer to the step-by-step guides for assistance on Corppass setup.\n",
      "2. Have your Singpass and your company’s Unique Entity Number (UEN)/ Entity ID ready.\n",
      "3. Visit the mytax.iras.gov.sg website to file the Corporate Income Tax Return for your company.\n",
      "4. If your company is filing Form C, you need to submit the financial statements/certified accounts and tax computation(s) for the relevant Year of Assessment (YA).\n",
      "5. If your company meets the qualifying conditions to file Form C-S or Form C-S (Lite), you can choose to file the simplified version, Form C-S (Lite), if your company has an annual revenue of $200,000 or below.\n",
      "\n",
      "You can also visit the Basic Guide to Corporate Income Tax for Companies page to get help with filing your company’s tax returns for the first time. Additionally, you can access the New Company Start-Up Kit, an interactive guide, to learn more.\n"
     ]
    }
   ],
   "source": [
    "print(chain_with_ranker.invoke(\"How do i file taxes for my company?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30083cbb",
   "metadata": {
    "id": "30083cbb",
    "outputId": "a517c0c1-21cf-4b66-bf32-0cc299513ea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided documents, if your foreign employee is injured at work, you can report the incident and claim reparations under the Work Injury Compensation Act (WICA). Specifically, the documents mention that input tax can be claimed for work injury compensation insurance that is obligatory under WICA for both local and foreign employees performing manual work or non-manual work earning $2,600 or less a month.\n",
      "\n",
      "To report the incident and make a claim, you can visit the Ministry of Manpower (MOM) webpage on WICA or contact MOM at +65 6438 5122. However, it is important to note that medical and accident insurance premiums for your staff are generally not allowable for input tax claims under the GST (General) Regulations, unless the insurance or payment of compensation is obligatory under WICA or under any collective agreement within the meaning of the Industrial Relations Act.\n",
      "\n",
      "Therefore, it seems that you can report the incident and claim reparations under WICA, but you should verify the specific requirements and conditions for your situation with MOM or your trade union.\n"
     ]
    }
   ],
   "source": [
    "print(chain_with_ranker.invoke(\"In the event my foriegn employee is injured at work, how do i report the incident and claim reparations?\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
