{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent Testing\n",
    "\n",
    "Testing on multi-agent system\n",
    "\n",
    "Current progress:\n",
    "\n",
    "Implement the 3 tools the model may use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r sgp_bootcamp_material/project/multi-agent-requirements.txt\n",
    "!pip install langgraph\n",
    "!pip install langchain-nvidia-ai-endpoints==0.3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g05/.local/lib/python3.9/site-packages/langchain_community/tools/tavily_search/__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.tools.tavily_search.tool import (\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "import getpass, os, base64\n",
    "from typing import Annotated, Literal\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from collections.abc import Iterable\n",
    "from random import randint\n",
    "from langgraph.prebuilt import InjectedState\n",
    "from langchain_core.messages.tool import ToolMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "assert nvidia_api_key.startswith(\"nvapi-\"), f\"{nvidia_api_key[:5]}... is not a valid key\"\n",
    "os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your tvly API key:  ········\n"
     ]
    }
   ],
   "source": [
    "tvly_api_key = getpass.getpass(\"Enter your tvly API key: \")\n",
    "assert tvly_api_key.startswith(\"tvly-\"), f\"{tvly_api_key[:5]}... is not a valid key\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = tvly_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Scripts\n",
    "\n",
    "https://www.kaggle.com/code/lejieng/day-3-building-an-agent-with-langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Conversation(TypedDict):\n",
    "    \"\"\"State representing the customer's conversation.\"\"\"\n",
    "\n",
    "    # The chat conversation. This preserves the conversation history\n",
    "    # between nodes. The `add_messages` annotation indicates to LangGraph\n",
    "    # that state is updated by appending returned messages, not replacing\n",
    "    # them.\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "    # Flag indicating that the order is placed and completed.\n",
    "    finished: bool\n",
    "    \n",
    "# The system instruction defines how the chatbot is expected to behave and includes\n",
    "# rules for when to call different functions, as well as rules for the conversation, such\n",
    "# as tone and what is permitted for discussion.\n",
    "SYSTEM_INSTRUCTIONS = (\n",
    "    \"system\",  # 'system' indicates the message is a system instruction.\n",
    "    \"You are a Legal Advice Chat Bot, you provide users with concise, accurate, and Singapore-specific legal information and guidance related to business operations, compliance, and regulations. \"\n",
    "    \"A human will ask you about any questions they regarding the legal domain relating to business and you will answer any questions \"\n",
    "    \"they have (and only about questions in the legal domain regarding business - no off topic discussions) \"\n",
    "    \"Only provide legal information relevant to Singapore’s business laws, corporate regulations, and compliance requirements. \"\n",
    "    \"Avoid discussing laws or business practices from other jurisdictions unless explicitly asked for comparison, and clarify that it is outside Singapore's context.\"\n",
    "    \"\\n\\n\"\n",
    "    \"Provide general guidance on topics such as company incorporation, employment regulations, tax compliance, intellectual property (IP), contract law, and data protection in Singapore.\"\n",
    "    \"Avoid offering personalized legal advice, contract drafting, or reviews. Encourage users to consult qualified professionals for specific situations.\"\n",
    "    \"\\n\\n\"\n",
    "    \"Business-specific topics include: Company incorporation and legal structures (e.g., Sole Proprietorship, LLP, Pte Ltd), \"\n",
    "    \"Employment regulations under the Employment Act, \"\n",
    "    \"Tax compliance (e.g., GST registration, corporate income tax), \"\n",
    "    \"Licensing and permits for businesses, \"\n",
    "    \"Data privacy laws, including compliance with the Personal Data Protection Act (PDPA), \"\n",
    "    \"Commercial contract fundamentals and enforceability, \"\n",
    "    \"and IP rights, including trademarks, copyrights, and patents in Singapore.\"\n",
    "    \"\\n\\n\"\n",
    "    \"Use straightforward, business-friendly language. \"\n",
    "    \"Explain legal concepts with examples or simplified analogies when possible. \"\n",
    "    \"Provide links to official Singapore government resources (e.g., ACRA, IRAS, MOM) where applicable. \"\n",
    "    \"\\n\\n\"\n",
    "    \"Ensure responses are aligned with the latest Singaporean laws, regulations, and best practices. \"\n",
    "    \"If unsure of current laws, advise users to verify with government authorities or legal experts. \"\n",
    "    \"\\n\\n\"\n",
    "    \"Do not provide guidance that could facilitate illegal activities or tax evasion. \"\n",
    "    \"Avoid speculating on outcomes of legal disputes or offering advice that requires knowledge of specific business circumstances. \"\n",
    "    \"\\n\\n\"\n",
    "    \"Politely decline if a query falls outside Singapore’s legal context or business-related scope. Redirect users to appropriate resources or professionals. \"\n",
    "    \"Clearly state when information is general and not a substitute for professional legal counsel. \"\n",
    "    \"\\n\\n\"\n",
    "    'Examples of Accepted Queries: Topics the bot can assist with include \"How do I register a private limited company in Singapore?\" \"What are the requirements for hiring foreign employees?\" \"Do I need to register for GST if my business turnover exceeds $1 million?\" and \"What steps should I take to trademark my business logo?\"'\n",
    "    'Examples of Declined Queries: Requests such as “Can you draft a shareholders’ agreement for me?” or “What are corporate tax laws in Hong Kong?” will be declined politely, with users advised to consult professionals or explore suitable external resources. Similarly, unethical queries, such as “How can I avoid CPF contributions for employees?” will be met with a clear explanation of the legal obligations in Singapore.'\n",
    "    \"\\n\\n\"\n",
    "    \"Remember to provide citation to any information you provide to the user. \"\n",
    "    \"If the information you retrieve is old, please fact chat the information with more recent sources to ensure the information is accurate. \"\n",
    ")\n",
    "\n",
    "# This is the message with which the system opens the conversation.\n",
    "WELCOME_MSG = \"Welcome to the ASPER LOVERS LEGAL Bot. Type `q` to quit. How may I serve you today?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mistral hates me\n",
    "# we ball with llama3.1-405b instruct\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3-8b-instruct\", base_url=\"http://localhost:8000/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def human_node(state: Conversation) -> Conversation:\n",
    "    \"\"\"Display the last model message to the user, and receive the user's input.\"\"\"\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    print(\"Model:\", last_msg.content)\n",
    "\n",
    "    user_input = input(\"User: \")\n",
    "\n",
    "    # If it looks like the user is trying to quit, flag the conversation\n",
    "    # as over.\n",
    "    if user_input in {\"q\", \"quit\", \"exit\", \"goodbye\"}:\n",
    "        state[\"finished\"] = True\n",
    "\n",
    "    return state | {\"messages\": [(\"user\", user_input)]}\n",
    "\n",
    "def chatbot_with_tools(state: Conversation) -> Conversation:\n",
    "    \"\"\"The chatbot with tools. A simple wrapper around the model's own chat interface.\"\"\"\n",
    "    defaults = {\"order\": [], \"finished\": False}\n",
    "\n",
    "    if state[\"messages\"]:\n",
    "        new_output = llm_with_tools.invoke([SYSTEM_INSTRUCTIONS] + state[\"messages\"])\n",
    "    else:\n",
    "        new_output = AIMessage(content=WELCOME_MSG)\n",
    "\n",
    "    # Set up some defaults if not already set, then pass through the provided state,\n",
    "    # overriding only the \"messages\" field.\n",
    "    return defaults | state | {\"messages\": [new_output]}\n",
    "\n",
    "def maybe_exit_human_node(state: Conversation) -> Literal[\"chatbot\", \"__end__\"]:\n",
    "    \"\"\"Route to the chatbot, unless it looks like the user is exiting.\"\"\"\n",
    "    if state.get(\"finished\", False):\n",
    "        return END\n",
    "    else:\n",
    "        return \"chatbot\"\n",
    "    \n",
    "def maybe_route_to_tools(state: Conversation) -> Literal[\"tools\", \"human\"]:\n",
    "    \"\"\"Route between human or tool nodes, depending if a tool call is made.\"\"\"\n",
    "    if not (msgs := state.get(\"messages\", [])):\n",
    "        raise ValueError(f\"No messages found when parsing state: {state}\")\n",
    "\n",
    "    # Only route based on the last message.\n",
    "    msg = msgs[-1]\n",
    "\n",
    "    # When the chatbot returns tool_calls, route to the \"tools\" node.\n",
    "    if hasattr(msg, \"tool_calls\") and len(msg.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"human\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VLM setup??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.9/site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.9/site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in ./.local/lib/python3.9/site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /cm/local/apps/python39/lib/python3.9/site-packages (from torch) (3.15.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /cm/local/apps/python39/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /cm/local/apps/python39/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.9/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.9/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.9/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.local/lib/python3.9/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.1.105)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.9/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.local/lib/python3.9/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /cm/local/apps/python39/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n",
      "\u001b[33mWARNING: Error parsing requirements for diagnose-kube: [Errno 2] No such file or directory: '/cm/local/apps/python39/lib/python3.9/site-packages/diagnose_kube-10.0.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/transformers@aae496dd154f72242a7d6ebeca132763d3dbc903\n",
      "  Cloning https://github.com/huggingface/transformers (to revision aae496dd154f72242a7d6ebeca132763d3dbc903) to /tmp/pip-req-build-zpdwjq3l\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-zpdwjq3l\n",
      "  Running command git rev-parse -q --verify 'sha^aae496dd154f72242a7d6ebeca132763d3dbc903'\n",
      "  Running command git fetch -q https://github.com/huggingface/transformers aae496dd154f72242a7d6ebeca132763d3dbc903\n",
      "  Running command git checkout -q aae496dd154f72242a7d6ebeca132763d3dbc903\n",
      "  Resolved https://github.com/huggingface/transformers to commit aae496dd154f72242a7d6ebeca132763d3dbc903\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate in ./.local/lib/python3.9/site-packages (1.1.1)\n",
      "Requirement already satisfied: filelock in /cm/local/apps/python39/lib/python3.9/site-packages (from transformers==4.45.0.dev0) (3.15.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.local/lib/python3.9/site-packages (from transformers==4.45.0.dev0) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.9/site-packages (from transformers==4.45.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /cm/local/apps/python39/lib/python3.9/site-packages (from transformers==4.45.0.dev0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /cm/local/apps/python39/lib/python3.9/site-packages (from transformers==4.45.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.9/site-packages (from transformers==4.45.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /cm/local/apps/python39/lib/python3.9/site-packages (from transformers==4.45.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.local/lib/python3.9/site-packages (from transformers==4.45.0.dev0) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.9/site-packages (from transformers==4.45.0.dev0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.9/site-packages (from transformers==4.45.0.dev0) (4.67.0)\n",
      "Requirement already satisfied: psutil in /cm/local/apps/python39/lib/python3.9/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./.local/lib/python3.9/site-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /cm/local/apps/python39/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /cm/local/apps/python39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.9/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /cm/local/apps/python39/lib/python3.9/site-packages (from requests->transformers==4.45.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /cm/local/apps/python39/lib/python3.9/site-packages (from requests->transformers==4.45.0.dev0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.9/site-packages (from requests->transformers==4.45.0.dev0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /cm/local/apps/python39/lib/python3.9/site-packages (from requests->transformers==4.45.0.dev0) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /cm/local/apps/python39/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "\u001b[33mWARNING: Error parsing requirements for diagnose-kube: [Errno 2] No such file or directory: '/cm/local/apps/python39/lib/python3.9/site-packages/diagnose_kube-10.0.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: qwen-vl-utils[decord] in ./.local/lib/python3.9/site-packages (0.0.8)\n",
      "Requirement already satisfied: av in ./.local/lib/python3.9/site-packages (from qwen-vl-utils[decord]) (13.1.0)\n",
      "Requirement already satisfied: packaging in /cm/local/apps/python39/lib/python3.9/site-packages (from qwen-vl-utils[decord]) (24.1)\n",
      "Requirement already satisfied: pillow in ./.local/lib/python3.9/site-packages (from qwen-vl-utils[decord]) (10.4.0)\n",
      "Requirement already satisfied: requests in /cm/local/apps/python39/lib/python3.9/site-packages (from qwen-vl-utils[decord]) (2.32.3)\n",
      "Requirement already satisfied: decord in ./.local/lib/python3.9/site-packages (from qwen-vl-utils[decord]) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in ./.local/lib/python3.9/site-packages (from decord->qwen-vl-utils[decord]) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /cm/local/apps/python39/lib/python3.9/site-packages (from requests->qwen-vl-utils[decord]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /cm/local/apps/python39/lib/python3.9/site-packages (from requests->qwen-vl-utils[decord]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.9/site-packages (from requests->qwen-vl-utils[decord]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /cm/local/apps/python39/lib/python3.9/site-packages (from requests->qwen-vl-utils[decord]) (2024.6.2)\n",
      "\u001b[33mWARNING: Error parsing requirements for diagnose-kube: [Errno 2] No such file or directory: '/cm/local/apps/python39/lib/python3.9/site-packages/diagnose_kube-10.0.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow-cpu in ./.local/lib/python3.9/site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./.local/lib/python3.9/site-packages (from tensorflow-cpu) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./.local/lib/python3.9/site-packages (from tensorflow-cpu) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./.local/lib/python3.9/site-packages (from tensorflow-cpu) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.local/lib/python3.9/site-packages (from tensorflow-cpu) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./.local/lib/python3.9/site-packages (from tensorflow-cpu) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./.local/lib/python3.9/site-packages (from tensorflow-cpu) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./.local/lib/python3.9/site-packages (from tensorflow-cpu) (3.4.0)\n",
      "Requirement already satisfied: packaging in /cm/local/apps/python39/lib/python3.9/site-packages (from tensorflow-cpu) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./.local/lib/python3.9/site-packages (from tensorflow-cpu) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /cm/local/apps/python39/lib/python3.9/site-packages (from tensorflow-cpu) (2.32.3)\n",
      "Requirement already satisfied: setuptools in ./.local/lib/python3.9/site-packages (from tensorflow-cpu) (75.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /cm/local/apps/python39/lib/python3.9/site-packages (from tensorflow-cpu) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.local/lib/python3.9/site-packages (from tensorflow-cpu) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /cm/local/apps/python39/lib/python3.9/site-packages (from tensorflow-cpu) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /cm/local/apps/python39/lib/python3.9/site-packages (from tensorflow-cpu) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.local/lib/python3.9/site-packages (from tensorflow-cpu) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in ./.local/lib/python3.9/site-packages (from tensorflow-cpu) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in ./.local/lib/python3.9/site-packages (from tensorflow-cpu) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in ./.local/lib/python3.9/site-packages (from tensorflow-cpu) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in ./.local/lib/python3.9/site-packages (from tensorflow-cpu) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in ./.local/lib/python3.9/site-packages (from tensorflow-cpu) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.local/lib/python3.9/site-packages (from tensorflow-cpu) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.local/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow-cpu) (0.45.0)\n",
      "Requirement already satisfied: rich in /cm/local/apps/python39/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow-cpu) (13.7.1)\n",
      "Requirement already satisfied: namex in ./.local/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow-cpu) (0.0.8)\n",
      "Requirement already satisfied: optree in ./.local/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow-cpu) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /cm/local/apps/python39/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /cm/local/apps/python39/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /cm/local/apps/python39/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (2024.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.local/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow-cpu) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.local/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow-cpu) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.local/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow-cpu) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /cm/local/apps/python39/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow-cpu) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /cm/local/apps/python39/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-cpu) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /cm/local/apps/python39/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow-cpu) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /cm/local/apps/python39/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow-cpu) (2.18.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /cm/local/apps/python39/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow-cpu) (3.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /cm/local/apps/python39/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-cpu) (0.1.2)\n",
      "\u001b[33mWARNING: Error parsing requirements for diagnose-kube: [Errno 2] No such file or directory: '/cm/local/apps/python39/lib/python3.9/site-packages/diagnose_kube-10.0.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# VLM setup\n",
    "# %pip uninstall torch torchvision torchaudio -y\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install git+https://github.com/huggingface/transformers@aae496dd154f72242a7d6ebeca132763d3dbc903 accelerate # this specific commit, not the latest for whatever reason?? idk i just plucked from the issue and PR\n",
    "%pip install qwen-vl-utils[decord]\n",
    "%pip uninstall -y tensorflow && pip install tensorflow-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "class QwenVLM:\n",
    "    def __init__(self):\n",
    "        # default: Load the model on the available device(s)\n",
    "        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            \"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        self.processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "\n",
    "    # The default range for the number of visual tokens per image in the model is 4-16384.\n",
    "    # You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n",
    "    # min_pixels = 256*28*28\n",
    "    # max_pixels = 1280*28*28\n",
    "    # processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "    @tool\n",
    "    def invoke_tool(prompt):\n",
    "        \"\"\"\n",
    "        Invokes call to model residing in local memory, see if we can serve elsewhere \n",
    "        \"\"\"\n",
    "        \n",
    "        return self.invoke(prompt)\n",
    "    \n",
    "    def invoke(prompt):\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"image\": \"sgp_bootcamp_material/project/DB/test2.png\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Describe the contents of this image.\"\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Preparation for inference\n",
    "        text = processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "\n",
    "        # Inference: Generation of the output\n",
    "        generated_ids = self.model.generate(**inputs, max_new_tokens=8196)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        output_text = self.processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        \n",
    "        return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def RAG_FROM_DATABASE():\n",
    "    \"\"\"Tool for searching through database to search for prior ingested information\n",
    "    \n",
    "    Returns:\n",
    "        PLACEHOLDER: The prior ingested information\n",
    "    \"\"\"\n",
    "    PLACEHOLDER = \"\"\"Everything is Illegal\"\"\"\n",
    "    \n",
    "    return PLACEHOLDER\n",
    "\n",
    "# @tool\n",
    "# def SEARCH_INTERNET():\n",
    "#     \"\"\"Tool for searching the internet in the event no relevant information can be found in ingested information database\n",
    "    \n",
    "#     Returns:\n",
    "#         PLACEHOlDER: Internet Information\n",
    "#     \"\"\"\n",
    "#     PLACEHOLDER = TavilySearchResults(max_results=1)\n",
    "    \n",
    "#     return PLACEHOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3ad1a19d364981bc822a836be552ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the tools and create a \"tools\" node.\n",
    "# tools = [USE_VLM, RAG_FROM_DATABASE, SEARCH_INTERNET]\n",
    "qwen = QwenVLM()\n",
    "tools = [qwen.invoke_tool, RAG_FROM_DATABASE, TavilySearchResults(max_results=1)]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# Attach the tools to the model so that it knows what it can call.\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder = StateGraph(Conversation)\n",
    "\n",
    "# Add the nodes, including the new tool_node.\n",
    "graph_builder.add_node(\"chatbot\", chatbot_with_tools)\n",
    "graph_builder.add_node(\"human\", human_node)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Chatbot may go to tools, or human.\n",
    "graph_builder.add_conditional_edges(\"chatbot\", maybe_route_to_tools)\n",
    "# Human may go back to chatbot, or exit.\n",
    "graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n",
    "\n",
    "# Tools always route back to chat afterwards.\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "test_graphs = graph_builder.compile()\n",
    "\n",
    "Image(test_graphs.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Welcome to the ASPER LOVERS LEGAL Bot. Type `q` to quit. How may I serve you today?\n",
      "User:  i love balls\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "[###] Unknown Error\n{'object': 'error', 'message': 'The model `meta/llama-3-8b-instruct` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mtest_graphs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langgraph/pregel/__init__.py:1918\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1917\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1918\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1920\u001b[0m     config,\n\u001b[1;32m   1921\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1922\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1923\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1924\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1925\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1927\u001b[0m ):\n\u001b[1;32m   1928\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1929\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langgraph/pregel/__init__.py:1646\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1641\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1645\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1646\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1647\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1648\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1649\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1650\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1651\u001b[0m         ):\n\u001b[1;32m   1652\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1653\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langgraph/pregel/runner.py:104\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    102\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, writer)\u001b[0m\n\u001b[1;32m     38\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langgraph/utils/runnable.py:410\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langgraph/utils/runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[14], line 20\u001b[0m, in \u001b[0;36mchatbot_with_tools\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     17\u001b[0m defaults \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinished\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 20\u001b[0m     new_output \u001b[38;5;241m=\u001b[39m \u001b[43mllm_with_tools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSYSTEM_INSTRUCTIONS\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     new_output \u001b[38;5;241m=\u001b[39m AIMessage(content\u001b[38;5;241m=\u001b[39mWELCOME_MSG)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_core/runnables/base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5350\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5356\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:382\u001b[0m, in \u001b[0;36mChatNVIDIA._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m inputs, extra_headers \u001b[38;5;241m=\u001b[39m _process_for_vlm(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    381\u001b[0m payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_payload(inputs\u001b[38;5;241m=\u001b[39minputs, stop\u001b[38;5;241m=\u001b[39mstop, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 382\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_req\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m responses, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mpostprocess(response)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_callback_out(responses, run_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_nvidia_ai_endpoints/_common.py:473\u001b[0m, in \u001b[0;36m_NVIDIAClient.get_req\u001b[0;34m(self, payload, extra_headers)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_req\u001b[39m(\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    469\u001b[0m     payload: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m    470\u001b[0m     extra_headers: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m    471\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m    472\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Post to the API.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m     response, session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(response, session)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_nvidia_ai_endpoints/_common.py:369\u001b[0m, in \u001b[0;36m_NVIDIAClient._post\u001b[0;34m(self, invoke_url, payload, extra_headers)\u001b[0m\n\u001b[1;32m    365\u001b[0m session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_session_fn()\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response \u001b[38;5;241m=\u001b[39m response \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__add_authorization(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_inputs)\n\u001b[1;32m    368\u001b[0m )\n\u001b[0;32m--> 369\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response, session\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_nvidia_ai_endpoints/_common.py:462\u001b[0m, in \u001b[0;36m_NVIDIAClient._try_raise\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    460\u001b[0m     body \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease check or regenerate your API key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# todo: raise as an HTTPError\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheader\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mbody\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: [###] Unknown Error\n{'object': 'error', 'message': 'The model `meta/llama-3-8b-instruct` does not exist.', 'type': 'NotFoundError', 'param': None, 'code': 404}"
     ]
    }
   ],
   "source": [
    "state = test_graphs.invoke({\"messages\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Got unknown type n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnigeria\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:378\u001b[0m, in \u001b[0;36mChatNVIDIA._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    371\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    375\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    376\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    377\u001b[0m         message\n\u001b[0;32m--> 378\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m [convert_message_to_dict(message) \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[1;32m    379\u001b[0m     ]\n\u001b[1;32m    380\u001b[0m     inputs, extra_headers \u001b[38;5;241m=\u001b[39m _process_for_vlm(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    381\u001b[0m     payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_payload(inputs\u001b[38;5;241m=\u001b[39minputs, stop\u001b[38;5;241m=\u001b[39mstop, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:378\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    371\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    375\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    376\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    377\u001b[0m         message\n\u001b[0;32m--> 378\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m [\u001b[43mconvert_message_to_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[1;32m    379\u001b[0m     ]\n\u001b[1;32m    380\u001b[0m     inputs, extra_headers \u001b[38;5;241m=\u001b[39m _process_for_vlm(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    381\u001b[0m     payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_payload(inputs\u001b[38;5;241m=\u001b[39minputs, stop\u001b[38;5;241m=\u001b[39mstop, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_nvidia_ai_endpoints/_utils.py:60\u001b[0m, in \u001b[0;36mconvert_message_to_dict\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m     54\u001b[0m     message_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: message\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_call_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: message\u001b[38;5;241m.\u001b[39mtool_call_id,\n\u001b[1;32m     58\u001b[0m     }\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unknown type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39madditional_kwargs:\n\u001b[1;32m     62\u001b[0m     message_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39madditional_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: Got unknown type n"
     ]
    }
   ],
   "source": [
    "llm.generate(\"nigeria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Welcome to the ASPER LOVERS LEGAL Bot. Type `q` to quit. How may I serve you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "can you call the vlm tool for debug purposes\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  USE_VLM (chatcmpl-tool-8e914b41695e49d49d51cad361bbacbb)\n",
      " Call ID: chatcmpl-tool-8e914b41695e49d49d51cad361bbacbb\n",
      "  Args:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: USE_VLM\n",
      "\n",
      "content=\"I'm sorry, but I am unable to view images or screenshots. I can only process text or provide information based on prompts and my programming. Is there something specific you would like to know or talk about?\" additional_kwargs={} response_metadata={'role': 'assistant', 'content': \"I'm sorry, but I am unable to view images or screenshots. I can only process text or provide information based on prompts and my programming. Is there something specific you would like to know or talk about?\", 'token_usage': {'prompt_tokens': 41, 'total_tokens': 89, 'completion_tokens': 48}, 'finish_reason': 'stop', 'model_name': 'microsoft/phi-3.5-vision-instruct'} id='run-2276fd36-8483-4911-9709-b9d9d0008a55-0' usage_metadata={'input_tokens': 41, 'output_tokens': 48, 'total_tokens': 89} role='assistant'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I am unable to view images or screenshots. I can only process text or provide information based on prompts and my programming. Is there something specific you would like to know or talk about?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "q\n"
     ]
    }
   ],
   "source": [
    "for chunk in state['messages']:\n",
    "    chunk.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1GPU_Kernel",
   "language": "python",
   "name": "1gpu_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
