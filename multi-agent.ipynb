{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent Testing\n",
    "\n",
    "Testing on multi-agent system\n",
    "\n",
    "Current progress:\n",
    "\n",
    "Implement the 3 tools the model may use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r sgp_bootcamp_material/project/multi-agent-requirements.txt\n",
    "!pip install langgraph\n",
    "!pip install langchain-nvidia-ai-endpoints==0.3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import getpass, os, base64\n",
    "from typing import Annotated, Literal\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from collections.abc import Iterable\n",
    "from random import randint\n",
    "from langgraph.prebuilt import InjectedState\n",
    "from langchain_core.messages.tool import ToolMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "assert nvidia_api_key.startswith(\"nvapi-\"), f\"{nvidia_api_key[:5]}... is not a valid key\"\n",
    "os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tvly_api_key = getpass.getpass(\"Enter your tvly API key: \")\n",
    "assert tvly_api_key.startswith(\"tvly-\"), f\"{tvly_api_key[:5]}... is not a valid key\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = tvly_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Scripts\n",
    "\n",
    "https://www.kaggle.com/code/lejieng/day-3-building-an-agent-with-langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Conversation(TypedDict):\n",
    "    \"\"\"State representing the customer's conversation.\"\"\"\n",
    "\n",
    "    # The chat conversation. This preserves the conversation history\n",
    "    # between nodes. The `add_messages` annotation indicates to LangGraph\n",
    "    # that state is updated by appending returned messages, not replacing\n",
    "    # them.\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "    # Flag indicating that the order is placed and completed.\n",
    "    finished: bool\n",
    "    \n",
    "# The system instruction defines how the chatbot is expected to behave and includes\n",
    "# rules for when to call different functions, as well as rules for the conversation, such\n",
    "# as tone and what is permitted for discussion.\n",
    "SYSTEM_INSTRUCTIONS = (\n",
    "    \"system\",  # 'system' indicates the message is a system instruction.\n",
    "    \"You are a Legal Advice Chat Bot, you provide users with concise, accurate, and Singapore-specific legal information and guidance related to business operations, compliance, and regulations. \"\n",
    "    \"A human will ask you about any questions they regarding the legal domain relating to business and you will answer any questions \"\n",
    "    \"they have (and only about questions in the legal domain regarding business - no off topic discussions) \"\n",
    "    \"Only provide legal information relevant to Singapore’s business laws, corporate regulations, and compliance requirements. \"\n",
    "    \"Avoid discussing laws or business practices from other jurisdictions unless explicitly asked for comparison, and clarify that it is outside Singapore's context.\"\n",
    "    \"\\n\\n\"\n",
    "    \"Provide general guidance on topics such as company incorporation, employment regulations, tax compliance, intellectual property (IP), contract law, and data protection in Singapore.\"\n",
    "    \"Avoid offering personalized legal advice, contract drafting, or reviews. Encourage users to consult qualified professionals for specific situations.\"\n",
    "    \"\\n\\n\"\n",
    "    \"Business-specific topics include: Company incorporation and legal structures (e.g., Sole Proprietorship, LLP, Pte Ltd), \"\n",
    "    \"Employment regulations under the Employment Act, \"\n",
    "    \"Tax compliance (e.g., GST registration, corporate income tax), \"\n",
    "    \"Licensing and permits for businesses, \"\n",
    "    \"Data privacy laws, including compliance with the Personal Data Protection Act (PDPA), \"\n",
    "    \"Commercial contract fundamentals and enforceability, \"\n",
    "    \"and IP rights, including trademarks, copyrights, and patents in Singapore.\"\n",
    "    \"\\n\\n\"\n",
    "    \"Use straightforward, business-friendly language. \"\n",
    "    \"Explain legal concepts with examples or simplified analogies when possible. \"\n",
    "    \"Provide links to official Singapore government resources (e.g., ACRA, IRAS, MOM) where applicable. \"\n",
    "    \"\\n\\n\"\n",
    "    \"Ensure responses are aligned with the latest Singaporean laws, regulations, and best practices. \"\n",
    "    \"If unsure of current laws, advise users to verify with government authorities or legal experts. \"\n",
    "    \"\\n\\n\"\n",
    "    \"Do not provide guidance that could facilitate illegal activities or tax evasion. \"\n",
    "    \"Avoid speculating on outcomes of legal disputes or offering advice that requires knowledge of specific business circumstances. \"\n",
    "    \"\\n\\n\"\n",
    "    \"Politely decline if a query falls outside Singapore’s legal context or business-related scope. Redirect users to appropriate resources or professionals. \"\n",
    "    \"Clearly state when information is general and not a substitute for professional legal counsel. \"\n",
    "    \"\\n\\n\"\n",
    "    'Examples of Accepted Queries: Topics the bot can assist with include \"How do I register a private limited company in Singapore?\" \"What are the requirements for hiring foreign employees?\" \"Do I need to register for GST if my business turnover exceeds $1 million?\" and \"What steps should I take to trademark my business logo?\"'\n",
    "    'Examples of Declined Queries: Requests such as “Can you draft a shareholders’ agreement for me?” or “What are corporate tax laws in Hong Kong?” will be declined politely, with users advised to consult professionals or explore suitable external resources. Similarly, unethical queries, such as “How can I avoid CPF contributions for employees?” will be met with a clear explanation of the legal obligations in Singapore.'\n",
    "    \"\\n\\n\"\n",
    "    \"Remember to provide citation to any information you provide to the user. \"\n",
    "    \"If the information you retrieve is old, please fact chat the information with more recent sources to ensure the information is accurate. \"\n",
    ")\n",
    "\n",
    "# This is the message with which the system opens the conversation.\n",
    "WELCOME_MSG = \"Welcome to the ASPER LOVERS LEGAL Bot. Type `q` to quit. How may I serve you today?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mistral hates me\n",
    "# we ball with llama3.1-405b instruct\n",
    "llm = ChatNVIDIA(model=\"meta/llama3-8b-instruct\", base_url=\"http://localhost:8000/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def human_node(state: Conversation) -> Conversation:\n",
    "    \"\"\"Display the last model message to the user, and receive the user's input.\"\"\"\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    print(\"Model:\", last_msg.content)\n",
    "\n",
    "    user_input = input(\"User: \")\n",
    "\n",
    "    # If it looks like the user is trying to quit, flag the conversation\n",
    "    # as over.\n",
    "    if user_input in {\"q\", \"quit\", \"exit\", \"goodbye\"}:\n",
    "        state[\"finished\"] = True\n",
    "\n",
    "    return state | {\"messages\": [(\"user\", user_input)]}\n",
    "\n",
    "def chatbot_with_tools(state: Conversation) -> Conversation:\n",
    "    \"\"\"The chatbot with tools. A simple wrapper around the model's own chat interface.\"\"\"\n",
    "    defaults = {\"order\": [], \"finished\": False}\n",
    "\n",
    "    if state[\"messages\"]:\n",
    "        new_output = llm_with_tools.invoke([SYSTEM_INSTRUCTIONS] + state[\"messages\"])\n",
    "    else:\n",
    "        new_output = AIMessage(content=WELCOME_MSG)\n",
    "\n",
    "    # Set up some defaults if not already set, then pass through the provided state,\n",
    "    # overriding only the \"messages\" field.\n",
    "    return defaults | state | {\"messages\": [new_output]}\n",
    "\n",
    "def maybe_exit_human_node(state: Conversation) -> Literal[\"chatbot\", \"__end__\"]:\n",
    "    \"\"\"Route to the chatbot, unless it looks like the user is exiting.\"\"\"\n",
    "    if state.get(\"finished\", False):\n",
    "        return END\n",
    "    else:\n",
    "        return \"chatbot\"\n",
    "    \n",
    "def maybe_route_to_tools(state: Conversation) -> Literal[\"tools\", \"human\"]:\n",
    "    \"\"\"Route between human or tool nodes, depending if a tool call is made.\"\"\"\n",
    "    if not (msgs := state.get(\"messages\", [])):\n",
    "        raise ValueError(f\"No messages found when parsing state: {state}\")\n",
    "\n",
    "    # Only route based on the last message.\n",
    "    msg = msgs[-1]\n",
    "\n",
    "    # When the chatbot returns tool_calls, route to the \"tools\" node.\n",
    "    if hasattr(msg, \"tool_calls\") and len(msg.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"human\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VLM setup??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VLM setup\n",
    "# %pip uninstall torch torchvision torchaudio -y\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install git+https://github.com/huggingface/transformers@aae496dd154f72242a7d6ebeca132763d3dbc903 accelerate # this specific commit, not the latest for whatever reason?? idk i just plucked from the issue and PR\n",
    "%pip install qwen-vl-utils[decord]\n",
    "%pip uninstall -y tensorflow && pip install tensorflow-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "class QwenVLM:\n",
    "    def __init__(self):\n",
    "        # default: Load the model on the available device(s)\n",
    "        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            \"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        self.processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "\n",
    "    # The default range for the number of visual tokens per image in the model is 4-16384.\n",
    "    # You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n",
    "    # min_pixels = 256*28*28\n",
    "    # max_pixels = 1280*28*28\n",
    "    # processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "    @tool\n",
    "    def invoke_tool(prompt):\n",
    "        \"\"\"\n",
    "        Invokes call to model residing in local memory, see if we can serve elsewhere \n",
    "        \"\"\"\n",
    "        \n",
    "        return self.invoke(prompt)\n",
    "    \n",
    "    def invoke(prompt):\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"image\": \"sgp_bootcamp_material/project/DB/test2.png\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Describe the contents of this image.\"\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Preparation for inference\n",
    "        text = processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "\n",
    "        # Inference: Generation of the output\n",
    "        generated_ids = self.model.generate(**inputs, max_new_tokens=8196)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        output_text = self.processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        \n",
    "        return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def RAG_FROM_DATABASE(query: str) -> str:\n",
    "    \"\"\"\n",
    "    A Retrieval-Augmented Generation (RAG) tool to answer a query using:\n",
    "    1. nv-embedqa-e5-v5 embeddings to embed the query.\n",
    "    2. Milvus vectorstore for similarity-based retrieval.\n",
    "    3. nv-rerankqa-mistral-4b-v3 reranker for improved result relevance.\n",
    "    4. Outputs the best response from the reranker.\n",
    "    \n",
    "    Parameters:\n",
    "        query (str): The user's input query.\n",
    "        \n",
    "    Returns:\n",
    "        str: The most relevant information retrieved from the database.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Specifiying embedding client\n",
    "    embedding_client = NVIDIAEmbeddings(\n",
    "      model=\"nvidia/nv-embedqa-mistral-7b-v2\", \n",
    "      truncate=\"NONE\", \n",
    "      )\n",
    "    \n",
    "    # Step 2: Embed the query\n",
    "    embeded_query =  embedding_client.embed_query(query)\n",
    "    \n",
    "    # Step 3: Specifying vectorstore and sending\n",
    "    DATA_API = \"http://10.149.8.40:9998\"\n",
    "    \n",
    "    # Step 4: Crafting request to milvus to get k(x) result. Output(list of results) = [\"result1\", \"result2\", ...]\n",
    "    milvus_response = requests.post(                    \n",
    "        DATA_API + \"/search\",\n",
    "        data={\"question\": query, \"embeded_question\": embeded_query, \"top_k\":10},\n",
    "        files={\"image\": uploaded_image.getvalue() if uploaded_image else None} \n",
    "        ).json()\n",
    "    \n",
    "    # Step 5: Specifying reranking model\n",
    "    rerank_client = NVIDIARerank(\n",
    "      model=\"nvidia/nv-rerankqa-mistral-4b-v3\"\n",
    "    )\n",
    "    \n",
    "    if milvus_response:\n",
    "        rerank_response = client.compress_documents(\n",
    "          query=query,\n",
    "          documents=[Document(page_content=passage.page_content, metadata=passage.metadata) for passage in milvus_response]\n",
    "        )\n",
    "    \n",
    "    # Step 6: Select top result (returns to model top result and cite)\n",
    "        if rerank_response:\n",
    "            top_result = rerank_response[0]\n",
    "            output = f\"Top Result: {top_result.content}\\n\\nCite: {top_result.metadata}\"\n",
    "            return output\n",
    "    else:\n",
    "        return \"No relevant results found for the query\"\n",
    "        \n",
    "    \n",
    "#    print(\"RAG from database tool called! Retrieving relevant items from milvus and neo4j...\") \n",
    "#    PLACEHOLDER = \"\"\"Everything is Illegal\"\"\"\n",
    "\n",
    "#    return PLACEHOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the tools and create a \"tools\" node.\n",
    "# tools = [USE_VLM, RAG_FROM_DATABASE, SEARCH_INTERNET]\n",
    "qwen = QwenVLM()\n",
    "tools = [qwen.invoke_tool, RAG_FROM_DATABASE, TavilySearchResults(max_results=1)]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# Attach the tools to the model so that it knows what it can call.\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(Conversation)\n",
    "\n",
    "# Add the nodes, including the new tool_node.\n",
    "graph_builder.add_node(\"chatbot\", chatbot_with_tools)\n",
    "graph_builder.add_node(\"human\", human_node)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Chatbot may go to tools, or human.\n",
    "graph_builder.add_conditional_edges(\"chatbot\", maybe_route_to_tools)\n",
    "# Human may go back to chatbot, or exit.\n",
    "graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n",
    "\n",
    "# Tools always route back to chat afterwards.\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "test_graphs = graph_builder.compile()\n",
    "\n",
    "Image(test_graphs.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state = test_graphs.invoke({\"messages\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Got unknown type n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnigeria\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:378\u001b[0m, in \u001b[0;36mChatNVIDIA._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    371\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    375\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    376\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    377\u001b[0m         message\n\u001b[0;32m--> 378\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m [convert_message_to_dict(message) \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[1;32m    379\u001b[0m     ]\n\u001b[1;32m    380\u001b[0m     inputs, extra_headers \u001b[38;5;241m=\u001b[39m _process_for_vlm(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    381\u001b[0m     payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_payload(inputs\u001b[38;5;241m=\u001b[39minputs, stop\u001b[38;5;241m=\u001b[39mstop, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:378\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    371\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    375\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    376\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    377\u001b[0m         message\n\u001b[0;32m--> 378\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m [\u001b[43mconvert_message_to_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[1;32m    379\u001b[0m     ]\n\u001b[1;32m    380\u001b[0m     inputs, extra_headers \u001b[38;5;241m=\u001b[39m _process_for_vlm(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    381\u001b[0m     payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_payload(inputs\u001b[38;5;241m=\u001b[39minputs, stop\u001b[38;5;241m=\u001b[39mstop, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/langchain_nvidia_ai_endpoints/_utils.py:60\u001b[0m, in \u001b[0;36mconvert_message_to_dict\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m     54\u001b[0m     message_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: message\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_call_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: message\u001b[38;5;241m.\u001b[39mtool_call_id,\n\u001b[1;32m     58\u001b[0m     }\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unknown type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39madditional_kwargs:\n\u001b[1;32m     62\u001b[0m     message_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39madditional_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: Got unknown type n"
     ]
    }
   ],
   "source": [
    "llm.generate(\"nigeria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Welcome to the ASPER LOVERS LEGAL Bot. Type `q` to quit. How may I serve you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "can you call the vlm tool for debug purposes\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  USE_VLM (chatcmpl-tool-8e914b41695e49d49d51cad361bbacbb)\n",
      " Call ID: chatcmpl-tool-8e914b41695e49d49d51cad361bbacbb\n",
      "  Args:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: USE_VLM\n",
      "\n",
      "content=\"I'm sorry, but I am unable to view images or screenshots. I can only process text or provide information based on prompts and my programming. Is there something specific you would like to know or talk about?\" additional_kwargs={} response_metadata={'role': 'assistant', 'content': \"I'm sorry, but I am unable to view images or screenshots. I can only process text or provide information based on prompts and my programming. Is there something specific you would like to know or talk about?\", 'token_usage': {'prompt_tokens': 41, 'total_tokens': 89, 'completion_tokens': 48}, 'finish_reason': 'stop', 'model_name': 'microsoft/phi-3.5-vision-instruct'} id='run-2276fd36-8483-4911-9709-b9d9d0008a55-0' usage_metadata={'input_tokens': 41, 'output_tokens': 48, 'total_tokens': 89} role='assistant'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I am unable to view images or screenshots. I can only process text or provide information based on prompts and my programming. Is there something specific you would like to know or talk about?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "q\n"
     ]
    }
   ],
   "source": [
    "for chunk in state['messages']:\n",
    "    chunk.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1GPU_Kernel",
   "language": "python",
   "name": "1gpu_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
